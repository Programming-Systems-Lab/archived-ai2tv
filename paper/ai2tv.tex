% $\mathrm{AI}^2$TV
% This is "sig-alternate.tex" V1.3 OCTOBER 2002
% This file should be compiled with V1.6 of "sig-alternate.cls" OCTOBER 2002
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V1.6 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ---------------------------------------------------------------------------
% This .tex file (and associated .cls V1.6) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 2002) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2003} will cause 2002 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the 
%  copyright line.
%
% ---------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.3 - OCTOBER 2002
\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ACM-MM 2004}{New York, NY USA}
%\CopyrightYear{2001} 
% Allows default copyright year (2000) to be over-ridden - IF NEED BE.

%\crdata{0-12345-67-8/90/01}  
% Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Using Workflow to Optimize QoS for Collaborative Client Video
Synchronization}
%% \title{Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
%% Format\titlenote{(Produces the permission block, and
%% copyright information). For use with
%% SIG-ALTERNATE.CLS. Supported by ACM.}}
%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{4}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Dan Phung\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{phung@cs.columbia.edu}
\alignauthor Giuseppe Valetto\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \affaddr{and Telecom Italia Lab}\\
       \affaddr{Turin, Italy}
       \email{valetto@cs.columbia.edu}
       \email{giuseppe.valetto@tilab.com}
\alignauthor Gail Kaiser \\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{kaiser@cs.columbia.edu}
}
\additionalauthors{Additional authors: Suhit Gupta {\texttt{suhit@columbia.cs.edu}}}
\date{\parbox[b][0ex]{0em}{\hspace*{-12.5em}\raisebox{37ex}{\fbox{For
submission to \emph{ACM-MM 2004}, due 12:00 AM EDT: April 05, 2004.}}}}
% \date{05 April 2004}
\maketitle

%% groups. Participants in these groups usually have a disparity
%% in network resources which can hamper productive dialog.  We 
\begin{abstract}
The increasing popularity of distance learning and online courses has
highlighted the lack of support for collaborative tools available for
student groups. We present a design and implementation of
$\mathrm{AI}^2$TV, a system that allows geographically dispersed
participants to collaboratively view a video in synchrony.  In
addition, this system uses an autonomic workflow controller that
responds to client feedback and adjusts the quality of the video
according to the resources of each client.  We show that our system
can successfully synchronize video for distributed clients and
optimize the video quality by adaptively adjusting the frame rate.

\end{abstract}

% A category with the (minimum) three required fields
\category{K.3.1}{Computer Uses In Education}{Collaborative learning}
\category{H.3.4}{Systems and Software}{Distributed systems}

\terms{???}

\keywords{Synchronized Collaborative Video, Autonomic Workflow}

% tech report number CUCS-009-04

% PAPER OUTLINE
% motivation:
% - online courses becoming more popular
% - limited support for distributed collaboration
% - network resource disparity
% -> thus, we give you ai2tv
%
% INTRO
% - show trend in online courses and current state on collaborative support
% - motivate the use of video as distributed collaboration tool
% - discuss briefly history of ai2tv design and imp
% - present rest of paper and summarize achievements
% 
% BACKGROUND
% - discuss other projects on multiple client synchronization
% - ai2tv project goals
% - describe overview of semantic compression tool used
% - introduce role of process workflow
% - explain time based synchronization
% 
% DESIGN AND IMPLEMENTATION
% - design motivations of ai2tv
% - workflow controller
% 
% EVALUATION methods
% - how to score synchronized state
% - define quality in terms of better frame rate
% - explain scoring system: decouple baseline evaluation and 
% penalties missed frames.
% 
% RESULTS
% - showed ai2tv w/ and w/o workflow each compares: 
%                : one graph for baseline comparison
%                : one graph for penalties
% 1) synchronization was successful
% 2) using workflow, QoS could be significantly increased
% 
% RELATED WORK
% - what are the other available online collaborative tools?
% - what are other methods for multiple client synchronization
% 
% CONCLUSION
% - our system can be used as an aid to support collaborative
% video viewing for students groups.  

% (does this need a name? SYCOVI (SYnchronized COllaborative Video?)

% final read = check for consistent tense

\section{Introduction}
A major shift in educational paradigms is the use of online courses,
which have become increasingly more popular \cite{BELLER,DOE}.  Due to
the ease in scheduling alongside a work schedule, one of the common
type of students attending online courses is represented by working
student that try to further her or his career through higher
certifications \cite{BURGESS:TRENDS}.  This student population usually
varies geographically, since online courses allow students from any
region to enroll.  The dispersed nature of the class prevents 
from physically holding group study sessions, which is commonly regarded by students as
a useful and productive practice.  Support for collaboration is a major concern in
courses where group work is encouraged \cite{WELLS:COLLAB}, yet there
are few tools that support distributed collaboration
\cite{BURGESS:TRENDS}.

The area of collaboration that our contribution applies to is
collaborative video viewing.  In this scenario, students that are
dispersed across different geographical regions can view lecture
videos simultaneously and in a synchronized manner: within a video viewing session, 
participants can play, pause, stop and go to certain frames synchronously as a group.
The intent is to facilitate the review of lecture material during distributed 
collaborative study sessions, in order, for instance, to solve some difficulty in a joint 
project assignment, or to clarify some notion.

% - motivate the use of video as distributed collaboration tool
Viewing video on the Internet requires relatively high bandiwdth
resources.  Network traffic is usually erratic, which can lead to lost
video content. Depending on the redundancy of video material, the lost content may or may not
contain significant information. 
To eliminate this uncertainty,
we use a semantic compression package \cite{TIECHENG} to extract 
relevant material and produce semantically rich videos .  
Using a selected time window, the package
summarizes the content over that time and extracts semantically rich
key frames.  We discuss this work more extensively in section
\ref{background}.

Under the abovementioned semantic compression scheme, the quality of
the video viewing expericence depends on the number of key frames
extracted, which is a function of the time window length. When only
the most semantically significant key frames are shown, the playback
of a video may seem unnatural.  In order to provide an appropriate
viewing experience to different clients, which may enjoy very
different bandwidth and computer resources, we instruct the semantic
compression algorithm to create more or less key frames depending on
the available bandwidth. \footnote{For performance reasons, we
actually precompute the key frames for several bandwidth levels and
thus make available a number of different versions of the same video
content, according to a hierarchy of semantic compression rates}.

% - briefly discuss ai2tv design and imp
In this paper we present our design and implementation of a
collaborative video viewing tool, developed in the context of a larger
project, called $\mathrm{AI}^2$TV (Adaptive Internet Interactive Team
Video).  Our tool exploits semantic compression, and employs a time
based synchronization scheme, a common event bus and an autonomic,
workflow-based controller to respond to feedback from each of the
clients invovled in a group viewing session, and fine tune their
viewing experience.  We show that using these mecahnisms, we are able
to synchronize content viewing across all clients, while ensuring an
improved video quality to each of the the clients.

% - present rest of paper and summarize achievements
In the rest of this paper, we present the background of previous
research that supports our work.  We then present the design and
implementation of $\mathrm{AI}^2$TV including the autonomic controller.
We then present the experimental design and results of the evaluation
of our system, and then we finish with related work and some conclusions.

\section{Background} \label{background}
% \section{The {\secit Body} of The Paper}
% - discuss other projects on multiple client synchronization
The advent of the Internet and the World Wide Web as a major means of
communication has presented the educational community with the
potential for great gains in the realm of accessibility to education.
Among applications that support online educational collaboration
(although they may not be geared specifically towards educational
purposes), there are instant message applications, application and
desktop sharing (WebEx, VNC), co-browsing \cite{CAPPS, LIEBERMAN,
SIDLER}, and XXX.  Those applications facilitate mostly the
communicative aspects of collaboration, supporting long-distance
exchange of ideas and know-how among pairs or groups of collaborators.
There is however a lack of tools that support also the collaborative
review and use of educational material produced for an on-line course.
Most distributed applications follow the metaphor of a Web portal,
which registered users access individually to fetch multimedia
material and complete assignments.  Only a few applications try to
support cooperation in an on-line study environment: for example, Easy
Teach and Learn proposes a virtual classroom architecture that allows
multiuser synchronous participation, but only a simulated version of
this system has been implemented \cite{WALTER}

% - ai2tv project goals
The work presented in this paper is a part $\mathrm{AI}^2$TV project,
a multi-group effort aimed at developing a collaborative virtual
environment for distributed team work.  The goal of $\mathrm{AI}^2$TV
is to provide networks of multimedia content relevant to the work
carried out by teams of users involved in the same project, such as
audio/video recordings of group discussions and decisions, and
informational and educational events.  $\mathrm{AI}^2$TV differs from
many existing infrastructures that provide educational content in a
networked context by its explicit focus on the teamwork aspects of
on-line education.  In $\mathrm{AI}^2$TV, a multimedia provisioning
subsystem is coupled with a collaborative Virtual Environment, named
CHIME\cite{CHIME}.

%% These participants may have varying bandwidth
%% resources from a 56k modem to broadband DSL (usually 1.5 Mbps download
%% bandwidth) or cable internet (usually 30 Mbps download bandwidth).

One of the requirements of the $\mathrm{AI}^2$TV multimedia subsystem
is the support for the synchronized viewing of streaming video by all
members of a geographically distributed team. To enforce the metaphor
of synchronous collaboration in the virtual environment, and for that
collaboration to be effective, all team members must be viewing the
same content at all times so that discussion of the material can be
coherent.  Synchronized viewing, however, is particularly difficult in
a context in which the multimedia content is to be delivered over
heterogeneous Internet links to heterogeneous platforms: team members
can be dispersed over the Internet, and may enjoy very diverse
connectivity, ranging for example from 56k modem, to DSL, to cable, to
T1 lines.  Moreover, in such a setup, the communication and computing
resources available to each user may widely and quickly vary in the
course of the team work session.

To address those issues and achieve synchronized viewing, we employ a
semantic summarization tool \cite{TIECHENG} that reduces a video to a
set of semantically significant key frames.

(???FIGURE: semantic compression )
(FIGURE: key frames hierarchy )

% - describe overview of semantic compression tool used
The semantic summarization algorithm profiles video frames within a
given time window and selects key frames that have the most semantic
information.  By increasing the size of the window, a key frame will
represent a larger time slice, which means that a larger window size
will produce less key frames as compared to a smaller window size
setting.  Thus, we use the window size as an indicator for the level
of semantic compression.  Semantic summarization is used to create
multiple versions of a video by specifying different levels of
semantic compression.  Note that at each level, even though the number
of key frames may differ, no semantic content is lost due to the
nature of the semantic summarization algorithm.  To prepare a video
for collaborative viewing, we pre-compute several sets of key frames,
obtaining a hierarchy of semantic compression levels.

For our purposes, we define an $\mathrm{AI}^2$TV video as the
compilation of key frames produced at the various different levels in
that hierarchy.  We use a key frame index file to access a certain
compression level of the $\mathrm{AI}^2$TV video.  Through the use of
the $\mathrm{AI}^2$TV video we can provide semantically similar
content to several clients with very diverse resources, by assigning
to each client a compression level in the hierarchy that matches the
overall amount of information transmitted to the available bandwidth.
Since the scheme we use, being based on the preservation of semantic
content, compresses information with a variable rate over time, and
since across the Internet the effective bandwidth of a network
connection is subject to fluctuations, we use an autonomic controller
to modify on the fly the video level assigned to each
$\mathrm{AI}^2$TV client in response to variations in network
conditions and information load.

% - introduce role of process workflow
The autonomic controller monitors the distributed $\mathrm{AI}^2$TV
system composed of clients and video servers, analyzes incoming
streams of monitoring data, and employs a workflow engine, named
\em{Workflakes}\cite{PEPPO}, to execute as needed workflow processes
that effect in a coordinated fashion the \em{dynamic adaptation} of
the video viewing settings across a group of clients.  That dynamic
adaptation aims at maintaining the clients in the same group
synchronized, in accordance with policies that implement an underlying
distributed synchronization scheme, while at the same time fine tuning
the quality of the viewing experience for individual clients.

It is noticeable that the adjustments required to carry out the
dynamic adaptation of $\mathrm{AI}^2$TV must be completed by the
Workflakes coordinator within narrow time boundaries (in the order of
one second or less), as mandated by the soft real-time nature of the
application and the tight synchronization effects that the dynamic
adaptation aims to achieve.

%% \subsection{Tables}
%% Because tables cannot be split across pages, the best
%% placement for them is typically the top of the page
%% nearest their initial cite.  To
%% ensure this proper ``floating'' placement of tables, use the
%% environment \textbf{table} to enclose the table's contents and
%% the table caption.  The contents of the table itself must go
%% in the \textbf{tabular} environment, to
%% be aligned properly in rows and columns, with the desired
%% horizontal and vertical rules.  Again, detailed instructions
%% on \textbf{tabular} material
%% is found in the \textit{\LaTeX\ User's Guide}.

%% Immediately following this sentence is the point at which
%% Table 1 is included in the input file; compare the
%% placement of the table here with the table in the printed
%% dvi output of this document.

%% \begin{table}
%% \centering
%% \caption{Frequency of Special Characters}
%% \begin{tabular}{|c|c|l|} \hline
%% Non-English or Math&Frequency&Comments\\ \hline
%% \O & 1 in 1,000& For Swedish names\\ \hline
%% $\pi$ & 1 in 5& Common in math\\ \hline
%% \$ & 4 in 5 & Used in business\\ \hline
%% $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%% \hline\end{tabular}
%% \end{table}

%% To set a wider table, which takes up the whole width of
%% the page's live area, use the environment
%% \textbf{table*} to enclose the table's contents and
%% the table caption.  As with a single-column table, this wide
%% table will ``float" to a location deemed more desirable.
%% Immediately following this sentence is the point at which
%% Table 2 is included in the input file; again, it is
%% instructive to compare the placement of the
%% table here with the table in the printed dvi
%% output of this document.


%% \begin{table*}
%% \centering
%% \caption{Some Typical Commands}
%% \begin{tabular}{|c|c|l|} \hline
%% Command&A Number&Comments\\ \hline
%% \texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
%% \texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
%% \texttt{{\char'134}table}& 300 & For tables\\ \hline
%% \texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
%% \end{table*}
%% % end the environment with {table*}, NOTE not {table}!

%% \subsection{Figures}
%% Like tables, figures cannot be split across pages; the best placement
%% for them is typically the top or the bottom of the page nearest their
%% initial cite.  To ensure this proper ``floating'' placement of
%% figures, use the environment \textbf{figure} to enclose the figure and
%% its caption.

%% This sample document contains examples of \textbf{.eps}
%% and \textbf{.ps} files to be displayable with \LaTeX.  More
%% details on each of these is found in the \textit{Author's Guide}.

%% \begin{figure}
%% \centering
%% % \epsfig{file=fly.eps}
%% \caption{A sample black and white graphic (.eps format).}
%% \end{figure}

%% \begin{figure}
%% \centering
%% % \epsfig{file=fly.eps, height=1in, width=1in}
%% \caption{A sample black and white graphic (.eps format)
%% that has been resized with the \texttt{epsfig} command.}
%% \end{figure}


%% As was the case with tables, you may want a figure
%% that spans two columns.  To do this, and still to
%% ensure proper ``floating'' placement of tables, use the environment
%% \textbf{figure*} to enclose the figure and its caption.
%% \begin{figure*}
%% \centering
%% \epsfig{file=flies.eps}
%% \caption{A sample black and white graphic (.eps format)
%% that needs to span two columns of text.}
%% \end{figure*}
%% and don't forget to end the environment with
%% {figure*}, not {figure}!

%% Note that either {\textbf{.ps}} or {\textbf{.eps}} formats are
%% used; use
%% the \texttt{{\char'134}epsfig} or \texttt{{\char'134}psfig}
%% commands as appropriate for the different file types.

%% \begin{figure}
%% \centering
%% \psfig{file=rosette.ps, height=1in, width=1in,}
%% \caption{A sample black and white graphic (.ps format) that has
%% been resized with the \texttt{psfig} command.}
%% \end{figure}

%% \subsection*{A {\secit Caveat} for the \TeX\ Expert}
%% Because you have just been given permission to
%% use the \texttt{{\char'134}newdef} command to create a
%% new form, you might think you can
%% use \TeX's \texttt{{\char'134}def} to create a
%% new command: \textit{Please refrain from doing this!}
%% Remember that your \LaTeX\ source code is primarily intended
%% to create camera-ready copy, but may be converted
%% to other forms -- e.g. HTML. If you inadvertently omit
%% some or all of the \texttt{{\char'134}def}s recompilation will
%% be, to say the least, problematic.

\section{Design} \label{design}

% design of a the system in general 
The design of our system involves several major components: a video
server, a number of video clients and an externalized autonomic
controller.
%
%(FIGURE: ai2tv synchronization arch)
%
The video server provides the multimedia educational content to the
clients for viewing.  The semantic compression package operates on
MPEG format videos and outputs sequences of JPG frames.  An
$\mathrm{AI}^2$TV video is produced by running off-line the tool
multiple times with settings for different compression levels; that
produces several sets of JPG frames, which are indexed by a frame
index file.  The task of the video server then is to simply provide
remote download access to these frames.

The task of video clients is to acquire video frames, display them at
the correct time, and provide a set of basic video functions,
specifically play, pause, goto, and stop.  Given the simple video
server model, this task can be achieved by keeping a video clock,
downloading the frames of the video for a given quality level, and
displaying a given frame at its representative time. The clocks of all
clients and the server are kept in sync using NTP \cite{NTP}, so that
each client can use its local clock with a guarantee that it refers to
a common time base. Taking a functional design perspective, the client
is thus composed of three major modules, devoted respectively to time
keeping, video fetching and caching, and video display. The role of
the cache element is particularly important, since it is in cahrge to
acquire the content to be shown and passes it over to the display
module. Also, as explained below, it is the locus where adaptation
takes place in the client.

The externalized autonomic controller provides the mechanisms that
allow client actions and video content to be synchronized across
multiple clients. The controller is itself a distributed system, whose
design derives from a reference conceptual architecture for autonomic
computing platforms proposed by Kaiser \cite{refarch}.  (XXX figure of
ref arch here). According to that architecture, \emph{sensors} attached
to elements of the target system of the autonomic platform
continuously collect data and send it to \emph{gauges}.  The role of
gauges is to analyze the data flow from multiple sensors and figure
out whether some adaptation is needed by the target system. If so,
they send a trigger to the controller's core, a coordination component
that is in charge to enact a set of corresponding changes onto
(possibly) multiple components of the target system.  To achieve that,
this coordinator orchestrates, according to a defined plan, the work
of a cohort of computations (referred to as \emph{effectors}) , which are
executed at the target system end. That coordination plan may need to
account for complex and dynamic sequencing and logic dependencies
among the various units of work represented by effectors' executions,
and may need to offer provisions for contingency planning and
compensations. The conceptual architecture does not indicate or
mandate a preferred approach to building the coordinator in a way that
complies with those requirements; however, a valid option to specify
and enact that kind of coordination plans - and the one experimented
with in the context of this work - is offered by workflow technology.

This autonomic architecture effectively provides a close control loop
that can be superimposed on a target system that does not have
built-in autonomic features.  it can take a reactive stance
(\emph{detect-and respond}), resulting in a feedback loop, and also a
proactive stance (\emph{detect-and-anticipate}), resulting in a
feed-forward loop.  The architecture remains also largely orthogonal
and disjoint from the target system and its own specifics, thus
representing a general-purpose tool that can be employed independently
from the application domain.

In $\mathrm{AI}^2$TV, the architecture described above collects
information on the state and resources of clients, gauges the
collective synchronization state of a group of clients, and employs
Workflakes to plan and bring about adjustments to individual clients
in the group to enforce synchronization and optimize the viewing
experience.

Communication within the distributed controller is provided by an
event communication bus based on the publish/subscribe paradigm (in
the current implementation of $\mathrm{AI}^2$TV, we use the
Siena\cite{Siena} event-based middleware).

Events transmitted onto the event bus include video actions, such as
pause and stop.  These video actions are time stamped so that clients
can respond to these directives in reference to the common time base.
Besides the occasional video actions, other events circulate on the
bus, emitted by sensors embedded in the cache of individual clients:
those events allow to monitor the state of each client with respect to
the acquired video conent and its ability to fetch and display that
content in time at some compression level in the hierarchy provided by
the $\mathrm{AI}^2$TV video.
%PEPPO - NOTE: here I would show an example of those events, i.e. its content in terms
%of attribute/value pairs with a bit of explanation.
%
Gauges that receive and process those events are embedded together
with the Workflakes coordinator, for expediency of design and to
minimize the communication latency when a trigger requesting some
adaptation must be sent from the gauges to Workflakes.  Gauges use a
set of helper functions that implement the $\mathrm{AI}^2$TV
synchronization scheme, to evaluate periodically (e.g. each second) if
a group of clients is in sync at that time, and, if not, to decide
which clients need to be adapted and how.  Adaptation directives,
based on those decisions and executed under the orchestration of
Workflakes, take once more the form of events that are sent by
Workflakes onto the bus, and are received by the caching module of
impacted clients.  Effectors is thus implemented as code within the
cache, which responds to incoming adaptation events by modifying how
the cache goes about fetching video content and handing it over to the
display module of the $\mathrm{AI}^2$TV client.

Notice how, by having sensors and effectors placed onto the cache of
the clients (as opposed to the display module), the autonomic
controller deals with video content that is about to be displayed
(rather than being currently displayed). That way, it puts in place a
detect-and-anticipate control loop, which takes preemptive rather than
reactive action; that is very important for our purposes, since we
want to avoid as much as possible situations that can lead to
synchronization faults, rather than recovering from them.

\section{Implementation} \label{implementation}

%Though the designs of both systems are not drastically different other
%than the synchronization architecture, the implementations of the two
%systems vary drastically other than the video server, which is simply
%a web server that provides access to the $\mathrm{AI}^2$TV video.
%
The $\mathrm{AI}^2$TV video display is implemented in Java and simply
renders the JPG frame in a window.  The video frame display controller
gets a list of available frames from the cache controller.  The
display client has a separate control for quality level that allows
the client to play at a certain quality level while the cache
controller downloads at a different level.  This feature allows the
workflow controller, described later in this section, to adjust the
client's display and cache separately and intelligently in response to
current bandwidth resources.

The cache controller is simply a downloading daemon that will continue
downloading at a certain quality level.  Directives to change the
quality level and other adjustments to the cache controller are
decided by the workflow controller.  The cache controller keeps a hash
of the available frames and a count of the current reserve frames
(frames buffered) for a given quality level.

%% and uses a thread sleep function to control the time
%% controller thread from hogging the processor.

The time keeping scheme uses the hardware clock as the base for its
internal video clock and  to ensure that the system clocks across
the $\mathrm{AI}^2$TV system are synchronized.  The separation of the
synchronization scheme greatly simplifies the rest of the system
because it clarifies the functional responsibilities of the other
components.  Each client plays the video frames at the correct time
and since all the clients have the same time, then all the clients
play in tandem.  Since the client is simply checking the time and
displaying the resepective video frame for that time, unless it is
misinformed about the correct frame to display, it will display the
correct video.  All video actions and workflow directives are
timestamped so the client can execute the actions in accordance with
that time.

The workflow controller makes decisions on behalf of the clients to
adjust their video quality levels according to collected statistics .
For each client, the workflow probes for the video display quality
level, cache controller quality level, cache controller reserve
frames, current frame and bandwidth.  The workflow directives can be
grouped into two categories: rules that adjust the client in response
to relatively low bandwidth situations and orders that take advantage
of relatively high bandwidth situations.  The gauge monitors the state
of the clients using the probe statistics described above and compares
them to configurable threshold values and mandates decisions
accordingly.

In the situation where a client has relatively low bandwidth, the
client may not download a frame in time.  This situation will merit
that both the client and the cache quality levels are reduced one
level.  In the case where the client is already at the lowest level,
the workflow controller will calculate the next possible frame that it
can successfully complete in time.

To take advantage of relatively high bandwidth situations, the cache
controller will accumulate a reserve buffer.  Once the buffer reaches
a threshold value, the workflow controller will direct the cache
controller to start downloading frames a higher quality level.  Once
the cache controller has downloaded a sufficient reserve at that
level, the client is then ordered to display the higher quality level.
Note that a higher quality level means a better frame rate, not a
better quality JPG.  If the bandwidth drops before the cache
controller can accumulate a sufficient buffer, then the cache
controller is dropped back down to the client level.  This incremental
increase in quality level prevents the client from entering a cycle
where it is repeatedly downgraded several levels due to a high
frequency change in frames.  There are pockets of high frequency
frames within the $\mathrm{AI}^2$TV video that reflects a portion of
the original video that has a large amount of semantic change.

The workflow controller allows the clients to take advantage or adjust
to relative deviations in bandwidth that occurs during a video
session.  Also, due to pockets of high frequency semantic change in
the video, the client must be carefully monitored during those times
to ensure that it doesn't drop those important frames.

\section{Evaluation Methods} \label{eval}

(FIGURE: show synchrony of how frames line up)

\textit{Evaluating Synchrony} \\ The purpose of the system is to
provide synchronous viewing to all clients.  To measure the
effectiveness of the synchrony, we probe the clients at and peroidic
time intervals and log the frame currently being displayed.  This
procedure effectively takes a snapshot of the system, which we can
evaluate for correctness.  This evaluation proceeds by checking
whether the frame being displayed at a certain time corresponds to one
of the valid frames at that time, on any arbitrary level.  (see
figure).  We allow any arbitrary level because the semantic
compression algorithm ensures that all frames at a certain time will
contain the same semantic information (???can we get kender to check
this, maybe a reference?) if the ``semantic windows'' overlap (???need
to get the correct term for this from kender).  We score the system by
summing the number of clients not showing an acceptable frame and
normalizing over the total number of clients.  A score of 0 indicates
a synchronized system.  We asses our system's using this evaluation
method on a selected set of client configurations, specifically, 1, 2,
3, 5 and 10 clients running a video for 5 minutes.  A two-tailed
t-test is used to measure the significance of any difference found.

\textit{Evaluating Quality of Service} \\
% since the semantic compression algorithm ensures that semantic information is preserved.
A major component of $\mathrm{AI}^2$TV is the workflow component whose
purpose is to increase the video quality for the clients.  For our
situation, a higher video quality means a higher frame rate.  Since we
are trying to measure how much better or worse the workflow adjusts
the clients, we must define a baseline client from which to compare.
To define the baseline client we will use a value that we identify as
the average bandwidth per level.  This value is computed by summing
the total size of the frames for a certain level and dividing by the
total video time.  This value provides the bandwidth needed on average
for the cache controller to download the next frame on time.

A baseline client also needs a context in which it is the baseline, so
we also use a bandwidth throttling mechanism \cite{shaperd} at the
server to dictate the bandwidth available to a certain client.  A
baseline client is thus defined as a client who is playing videos at
the quality level matching that of its bandwidth, which is found by
comparing the average bandwidth per level and the alloted bandwidth
and that level which has a needed bandwidth with a minimal positive
difference with the alloted bandwidth (the client just has enough
bandwidth to maitain that level).

To attain a quantitative measure of the quality of service provided by
a workflow assisted client, we use a scoring system relative to the
baseline client's quality level.  We give a score of 1 for each
quality level above the baseline quality level and -1 for each quality
level below.  Theoretically, the baseline client should receive a
score of 0.

%% The possible reasons for missing frames are an abrupt change in
%% bandwidth or a high frequency change in frames

The act of running the client close to or at a level higher than the
average bandwidth needed puts the client at risk for missing more
frames because the workflow is trying to push the client to the best
level.  To measure whether the workflow assisted client is exposed to
a higher risk of missing frames we also count the number of missed
frames during a video session.  The scoring of the missed frame is a
simple count of the missed frames.  Note that the scoring of the
missed frame is kept separate from the measure of the relative quality
to discriminate between levels of concern, though they both indicate a
characteristic of quality of service.

To assess the quality of service the workflow confers we evaluate the
log files using this scoring scheme for a selected set of client
configurations, specifically, 1, 2, 3, 5 and 10 clients running a
video for 5 minutes.  We run the system once without workflow, and
then again with the workflow engaged.  Again we sum and normalize the
score for all clients and use a two-tailed t-test is used to measure
the significance of any difference found.

%% Sanity checks to measure the correctness of the synchronization was
%% done through visual checks of the video frames to see if they were
%% indeed showing the purported frame.

\section{Results} \label{results}

\section{Discussion} \label{discussion}

\section{Related Work} \label{related}

Other work being done on CSCW schemes?  Other multiclient synchronous 
systems (MOOs and MUDs), what else?

\section{Future Work} \label{con}

Try a push model with the server.  Integrate streaming audio.

\section{Conclusions}

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}

Professor Kender and Tiecheng Liu at the High-Level Vision Lab.
Matias Pelenur also helped.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ai2tv}
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%

\subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.

% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.

\balancecolumns % GM July 2000
% That's all folks!
\end{document}
