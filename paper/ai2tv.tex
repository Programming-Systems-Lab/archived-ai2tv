% $RCSfile$
% $Revision$
% $Date$
% $Source$
%
%
% ---------------------------------------------------------------------------
% TODO:
%
% FINAL READ 
% - check for consistent tense
% - query replace: ai2tv -> $\mathrm{AI}^2$TV
% - spell check
%
%
% ---------------------------------------------------------------------------
% This is "sig-alternate.tex" V1.3 OCTOBER 2002
% This file should be compiled with V1.6 of "sig-alternate.cls" OCTOBER 2002
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V1.6 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ---------------------------------------------------------------------------
% This .tex file (and associated .cls V1.6) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 2002) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2003} will cause 2002 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the
%  copyright line.
%
% ---------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.3 - OCTOBER 2002
\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ACM-MM 2004}{New York, NY USA}
%\CopyrightYear{2001}
% Allows default copyright year (2000) to be over-ridden - IF NEED BE.

%\crdata{0-12345-67-8/90/01}
% Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Optimizing Quality for Collaborative Video Viewing}
%% \title{Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
%% Format\titlenote{(Produces the permission block, and
%% copyright information). For use with
%% SIG-ALTERNATE.CLS. Supported by ACM.}}
%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{4}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Dan Phung\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{phung@cs.columbia.edu}
\alignauthor Giuseppe Valetto\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \affaddr{and Telecom Italia Lab}\\
       \affaddr{Turin, Italy}
       \email{valetto@cs.columbia.edu}
\alignauthor Gail Kaiser \\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{kaiser@cs.columbia.edu}
}
\additionalauthors{Additional authors: Suhit Gupta {\texttt{suhit@columbia.cs.edu}}}
\date{\parbox[b][0ex]{0em}{\hspace*{-12.5em}\raisebox{37ex}{\fbox{For
submission to \emph{ACM-MM 2004}, due 12:00 AM EDT: April 05, 2004.}}}}
% \date{05 April 2004}
\maketitle

%% groups. Participants in these groups usually have a disparity
%% in network resources which can hamper productive dialog.  We

\begin{abstract}
The increasing popularity of distance learning and online courses has
highlighted the lack of support for collaborative tools available for
student groups.  In addition, the introduction of lecture videos into
the online cirriculum has draw attention to the disparity in the
network resources used by the students.  We present a design and
implementation of ai2tv, a system that allows participants that are
disadvantaged in network resources and geographically dispersed to
collaboratively view a video in synchrony.  We uphold the invariant
that each participant will view semantically similar content at all
times.  Video actions can be initiated by any of the participants and
the results of those actions are seen by all the members.  These
features allow group members to review lectures videos in tandem to
faciliate the learning process.  We employ an autonomic controller
that monitors client video status and adjusts the quality of the video
according to the resources of each client.  We show that our system
can successfully synchronize video for distributed clients and
optimizes the video quality by adaptively adjusting the quality level
for each participant.
\end{abstract}

% A category with the (minimum) three required fields
\category{C.2.4}{Distributed Systems}{Client/server, Distributed applications}
\category{D.2.8}{Software Engineering}{Metrics -- performance measures}
\category{H.5.1}{Information Interfaces and Presentation}{Multimedia Information Systems}
\category{H.5.3}{Group and Organization Interfaces}{Computer-supported cooperative work, Synchronous interaction}
\category{K.3.1}{Computer Uses In Education}{Collaborative learning, Distance learning}

\terms{ALGORITHMS, MEASUREMENT, PERFORMANCE, EXPERIMENTATION, HUMAN
FACTORS}

\keywords{Synchronized Collaborative Video, Autonomic Controller}

% tech report number CUCS-009-04
\section{Introduction}
A major shift in educational paradigms is the use of online courses,
which have become increasingly more popular \cite{BELLER,DOE}.  Due to
the ease in scheduling class work alongside a work schedule, one of
the common type of students of online courses are the non-traditional
working students that try to further their career through higher
certifications \cite{BURGESS}.  This student population is usually
geographically dispersed, since online courses allow students from any
region to enroll.  The educational tools commonly provided for these
students are accessed through a personalized Web portal
\cite{PHOENIX,CAPELLA}.

% need to 
To cater to the growing population of online students, universities
began taping on-campus course lectures and providing them to
off-campus students \cite{CVN, STANDFORD}.  These courses then
included both on and off campus students.  The latter population
introduced the term of distanced learning to describe students
enrolled in a local program yet attending the courses off-campus.
With the addition of lecture videos to the available online materals,
online students have access to a greater wealth of information.

Traditionally, the courses intended for online students are geared
towards individual learning paradigms.  The introduction of a whole
new set of courses from on-campus lectures introduced courses that are
sometimes designed for group work.  Also, because on-campus students
are in the same class as off-campus studnets, the new courses mixed in
a population of students who like to study in groups, a practice
commonly urged by professors to develop learning skills.  This shift
highlighted the lack of support for group collaboration.  Support of
synchronous collaboration remains a major concern in courses where
group work is encouraged \cite{WELLS}, yet there are few educational
tools that support synchronous collaboration across a group of online
students \cite{BURGESS}.

%% For example, group study sessions, which are regarded by students as a
%% useful and productive practice \cite{WELLS}, are physically prevented
%% from collaborating, a problem that cannot be sufficiently reconciled
%% with the Web portal metaphor.  

Our work contributes to the area of synchronous collaboration support,
and, more specificially, to collaborative video viewing.  In this
scenario, students that are dispersed across different geographical
regions can view lecture videos simultaneously and in a synchronized
manner: within a video session, participants can play, pause, stop and
go to certain frames synchronously as a group.  Therefore a WYSIWIS
(What You See Is What I See) invariant is upheld for all participants.
The goal is to support group review of lecture video so that if a
participant has a question about a certain topic, he/she can pause the
video and ask questions to other participants and be assured that all
the participants are referencing the same material.

% - motivate the use of video as distributed collaboration tool
Viewing video on the Internet requires relatively high bandiwdth
resources.  Network traffic is usually erratic, which can lead to lost
video content.  Furthermore, the differences of network and computing
resources available to dispersed users in the same collaboration group
can be very signficant.  Collaborative video viewing poses a twofold
problem: on the one hand, it is mandatory to keep all users
synchronized with respect to the content they are supposed to see at
any moment during play time; on the other hand, it is important to
provide each individual user with a level of viewing experience that
is appropriate with respect to the user's available resources, and to
dynamically take in account variations in those resources.

% - briefly discuss ai2tv design and imp
In this paper we present a system to satisfy both of the above
mentioned requirements for collaborative video viewing.  Our solution
employs a time based synchronization scheme and an autonomic control
entity.  The autonomic controller tunes the working parameters of the
client's video player in order to dynamically adjust the video quality
levels of the various users in a group.  We show that it is possible
to synchronize content viewing across all clients while ensuring an
high video quality to each of the the clients.

% - present rest of paper and summarize achievements
In the rest of this paper, we present the motivating problem and
introduce background research that supports this work.  Next, we
explain in detail the architecture and the approach taken in our
solution within the ai2tv project.  We then present the criteria used
to evaluate the effectiveness of the approach, together with the
corresponding results, and we finish with related work and a summary
of our contributions.

\section{Motivating problem} \label{background}
% - discuss other projects on multiple client synchronization
The advent of the Internet and the World Wide Web as a major means of
communication has presented the educational community with the
potential for great gains in the realm of accessibility to education
through eLearning.  A variety of eLearning applications that support
individual study practices exist, and typically follow the metaphor of
a Web portal where registered users can individually browse class
material or post completed assignments.  Several other applications
can be used to provide some level of support to online educational
collaboration, although they may not be geared specifically towards
educational purposes.  Among them, some are geared towards
asynchronous forms of collaboration, (for example Web-based bulletin
boards and discussion groups); others, such as instant messaging,
application and desktop sharing (WebEx, VNC), and co-browsing
\cite{CAPPS, LIEBERMAN, SIDLER} facilitate mostly the communicative
aspects of synchronous collaboration, such as the long-distance
sharing of ideas and knowledge among their users.  However, there are
few tools that provide an on-line study environment to support
synchronous collaborative study practices, such as the group review of
educational material produced for on-line courses \cite{WELLS}.  For
example, Easy Teach and Learn proposes a virtual classroom
architecture that allows multiuser synchronous participation, but only
a simulated version of this system has been implemented \cite{WALTER}.

% - ai2tv project goals
The work presented in this paper is a part of the ai2tv (Adaptive
Internet Interactive Team Video) project, a multi-group effort aimed
at developing a collaborative virtual environment for distributed team
study.  ai2tv differs from many existing infrastructures that provide
educational content in a networked context because of its explicit
focus on the teamwork aspects of on-line education, and hence on
synchronous collaboration.  Within ai2tv, an important goal is to
provide support for the collaborative use of multimedia content
relevant to the work carried out by a team of users involved in the
same project, such as audio/video recordings of lectures, meetings,
workshops and other informational and educational events.

% VECTORS (sys 1) and CHIME are mentioned in related work.
One of the requirements of the multimedia support within the ai2tv
virtual environment is the synchronized viewing of streaming video by
all members of a geographically distributed team.  To enforce the
metaphor of synchronous collaboration in a virtual environment and for
collaboration to be effective, all team members must be viewing the
same content at all times so that discussion of the material can be
coherent.  Synchronized viewing, however, is particularly difficult in
an Internet context: dispersed team members may enjoy very diverse
connectivity, ranging for example from 56k modem, to DSL, to cable, to
T1 lines.  Moreover, the communication and computing resources
available to each user may widely and quickly vary in the course of
the team work session.

% this sounds like our solution is the only solution
%% In such conditions, some form of adaptive control of the mechanisms
%% used to distribute and present the multimedia content to all team
%% members is clearly necessary.  
To deal with these constraints, we adaptively control the distribution
and presentation of the multimedia to all team members.  To be usable,
that control must achieve synchronized viewing without unreasonably
constraining the quality of the viewing experience of individual
users; for example, a "worst case scenario" in which the experience of
all members in a team is degraded to match that of the member with the
least resources is not acceptable. On the contrary, any valid solution
must reconcile the mandatory requirement of synchronized viewing with
an efficient use of the resources of individual users.

One solution to the problem of balancing the group synchronization
requirement with the optimization of individual viewing experiences is
to use videos with cumulative layering \cite{MCCANNE}, also known as
scalable coding \cite{LI}.  In this approach, the client software is
instructed to display video content at a quality level appropriate for
that client's resources, which is selected from a hierarchy of several
different encodings or frame rates for that video.

% - describe overview of semantic compression tool used
Either fixed rate or variable rate schemes can be equally employed. In
ai2tv, we take advantage of a semantic summarization package - also
developed at Columbia University by Liu and Kender \cite{TIECHENG} -
which reduces a video to a set of semantically significant key frames.
That tool operates on MPEG format videos and outputs sequences of JPG
frames.  Its semantic compression algorithm profiles video frames
within a sliding time window and selects key frames that have the most
semantic information.  By increasing the size of the window, a key
frame will represent a larger time slice, which means that a larger
window size will produce less key frames as compared to a smaller
window size setting.  

The semantic compression package is used to produce the layered videos
provided to clients.  Note that the semantic compression algorithm
produces a random distribution of key frames, hence the video produced
by the package plays back at a variable frame rate.  The variability
in the frame rate implies that there are pockets of relatively high
frequency semantic change that result in sections in the video that
demand a higher frame rate.  The variable frame rate video adds
complexity to the bandwidth demands of the client yet its semantic
focus ensures that relevant content is unlikely to get lost, which is
a significant property in the context of an educational application.
Through the use of the ai2tv video, we can provide semantically
equivalent content to several clients with diverse resources by
adjusting the compression level assigned to each client during playing
time.  Thus for our purposes, synchronization of video boils down to
showing semantically equivalent frames for a given time (see figure
???).

To adjust the clients in response to the changing environment, we use
an autonomic controller to maintain the synchronization of the group
of video clients while fine tuning the video quality for each client.
The term \textit{autonomic} as used here simply refers to the system's
ability to self-adjust to network conditions and can be seen as a form
of feedback control.  The idea of using an autonomic controller to
support group video synchronization and other multimedia apps was
first introduced in \cite{EWSPT}.

The autonomic controller remains conceptually separate from the
controlled ai2tv video system and employs a software based workflow
engine, named Workflakes \cite{PEPPO}.  ??? Note that the workflow
used here is software based as opposed to human based workflow
systems.  The use of software based workflow for the specification and
enactment of the plan that coordinates actuators is taken from Wise at
al. \cite{OSTERWEIL} among others.  The Workflakes engine has been
developed for and used in a variety of domains \cite{AMS,ICSE}, in
which it orchestrates the work of software entities to achieve the
fully automated dynamic adaptation of distributed applications.  The
design of the autonomic controller is a part of an externalized
autonomic computing platform proposed by Kaiser \cite{refarch}.  In
the context of ai2tv, Workflakes coordinates the adjustment of the
compression level assigned to each client along the hierarchy of the
ai2tv video.

(???FIGURE: semantic compression )
(???FIGURE: key frames hierarchy )

\section{Architecture and Model} \label{design}
\subsection{System Architecture}
% Design of a the system in general
Our system involves several major components: a video server, video
clients, an externalized autonomic controller and a common
communications infrastructure.

%(FIGURE: ai2tv synchronization arch)
% video server
The video server provides the educational video content to the clients
for viewing.  The provided content has the form of an ai2tv video,
i.e., a hierarchy of video versions produced by running the tool
multiple times with settings for different compression levels, which
produces several sets of JPG frames that are indexed by a frame index
file.  For each frame in the index file, a time interval
\texttt{[start:end]}, is provided that represents the valid times in
which the frame can be played.  The task of the video server is simply
to provide remote download access to the frames and the index file
over http.

% video client
The task of video clients is to acquire video frames, display them at
the correct time, and provide a set of basic video functions.  Taking
a functional design perspective, the client is composed of three major
modules: a video display, a video buffer and manager for fetching and
storing downloaded frames, and a time controller.

The video display renders the JPEG frames into a window for display
and provides a user interface for play, pause, goto, and stop.  When
any participant initiates one of these actions, all the other group
members receive the same command, thus all the video actions are
synchronized.  The video display knows which frame to display by using
the current video time and display quality level to index into the
frame index for the representative frame.  Before trying to render the
frame, it asks the video buffer manager if the needed frame is
available.  The video display also includes a control entity that
enables external entities, like the autonomic controler, to adjust the
current display quality level.

The video buffer and manager are the downloading daemon that
continuously downloads frames at a certain level.  It keeps a hash of
the available frames and a count of the current reserve frames (frames
buffered) for each quality level.  The buffer manager also includes a
control hook that enables external entities to adjust the current
downloading quality level.

The time controller's task is to ensure that a common video clock is
maintained across clients.  It relies on NTP \cite{NTP} to synchronize
the system's software clock therefore ensuring a common time base from
which each client can reference for the video clock.  The task of each
then is to play the frames at the correct time and since all the
clients refer to the same time base, then all the clients are showing
semantically similar frames.

% autonomic controller
The task of the autonomic controller is to ensure that the clients
within a video session stay synchronized and that each client plays at
its highest attainable quality level.  The controller is itself a
distributed system, whose design derives from a conceptual reference
architecture for externalized autonomic computing platforms proposed
by Kaiser \cite{refarch}.
%
%(??? figure of ref arch here?). 
%
According to that architecture, sensors attached to the end targets
continuously collect data and send it to gauges.  The sensors provide
the autonomic controller with data about the clients.  The data
collected includes the video display quality level, the buffer quality
level, the buffer reserve frames, the currently displayed frame and
the current bandwidth.  

Gauges are embedded together with the coordination engine for
expediency of design and to minimize the communication latency to it.
They receive the sensor reports from individual clients, collect them
in buckets, similar to the approach in \cite{MIMAZE}, and pass the
bucket data structure to the coordination engine.  The coordination
engine directs the flow of the information through a pre-defined
workflow plan described in the next section.

During the evaluation of the data, a set of helper functions that are
tailored specifically for the application are used to produce the
triggers for the coordinator.  If a trigger is raised, the
coordination engine enacts an adaptation scheme which is executed on
the end hosts by hooks provided to the actuators by the end systems.

% communications 
The infrastructure used for the communications among the video
clients, as well as between the ai2tv system and the autonomic
controller is provided by an event bus based on the publish/subscribe
paradigm.  The reason for choosing this communication model is that it
inherently decouples the physical location of the communicating
entities.  Events transmitted onto that event bus are of three kinds:
video actions, sensor reports and adaptation directives.  Video
actions pertain to the functionality of the ai2tv system, since they
represent commands issued on a video client, such as pause, play or
stop, which need to be propagated to all clients in the group to
enforce the same behaviour.  All video actions are time stamped so
that clients can respond to those commands in reference to the common
time base.

\subsection{Adaptation Scheme}

The adaptation scheme falls into two levels: a higher level data flow,
and a lower level adjustment heuristic.  The former directs the flow
of data through a logical sequence to provide a formal decision
process while the latter provides the critera as to when to make
certain adjustments.

The higher level logic is shown in Figure ???, according to the
Little-JIL graphic workflow specification language \cite{L-JIL}.  The
diagram shows the task decomposition hierarchy according to which the
adaptation workflow unfolds.  Note that the evaluation of clients'
state with respect to the group (\texttt{EvaluateClient}) and the
issuing of adaptation directives (\texttt{AdaptClient}) is carried out
as a set of the parallel steps.  Also note that the multiplicity of
those parallel steps is dynamically determined via the number of
entries in the \texttt{client} variable, which maps to a collection of
ai2tv clients.

%
%add Figure with AI2TV workflow diagram here
%

The adaptation scheme at the lower level falls into two categories:
directives that adjust the client in response to relatively low
bandwidth situations, and those that take advantage of relatively high
bandwidth situations.

In the situation where a client has relatively low bandwidth, the
client may not be able download next frame at the same quality level
in time.  This situation will merit that both the client and the
buffer quality levels are reduced one level. In the case in which the
client is already at the lowest level, the controller will calculate
the next possible frame that it can successfully complete in time - in
order to remain synchronized with the rest of the team - and will ask
the client to jump ahead to that frame.

To take advantage of relatively high bandwidth situations, the buffer
manager will start to accumulate a reserve buffer.  Once the buffer
reaches a threshold value (for example, 10 buffered frames), the
autonomic controller will direct the buffer manager to start fetching
frames a higher quality level.  Once a sufficient reserve is
accumulated also at that higher level, the client is then ordered to
display frames at that quality level.  If the bandwidth drops before
the buffer manager controller can accumulate enough frames in the
higher-level reserve, then the buffer manager is dropped back down one
level.

\section{Implementation} \label{implementation}

For simplicity, we used Java to implement the video client.  It
naively uses the javax.swing package to render the JPEG images.  The
autonomic controller, Workflakes, is also Java based and uses Cougaar
\cite{Cougaar} as the workflow engine.  We used the Little-JIL graphic
workflow specification language to produce the workflow used
\cite{L-JIL}.  We chose a content-based publish-subscribe event
system, Siena, as our communication bus \cite{SIENA}.

\section{Evaluation} \label{eval}

We asses our system by evaluating its ability to synchronize the
clients and to provide a high quality of video.  The evaluation
criteria presented below was computed from a set of client
configurations, specifically 1, 2, 3, and 5 clients running a video
for 5 minutes and probing system state every 5 seconds.

%% To evalute our system, we produced an ai2tv video that had 5 quality
%% levels.  For a 17 minute video and five different window lengths, the
%% total number of frames are 165, 71, 39, 21, and 13.  Our choice of the
%% relatively low frame rate quality levels was influenced by the goal of
%% the system being used by clients with low bandwidth resources.
 
% the pathetic average frame rates (per minute!!!):
%% 3.399831413 - high
%% 1.46295776
%% 0.806289939
%% 0.434237734
%% 0.268763313 - low

\textit{Evaluating Synchrony} \\ The purpose of the system is to
provide synchronous viewing to all clients.  To measure the
effectiveness of the synchrony, we probe the clients at peroidic time
intervals and log the frame currently being displayed.  This procedure
effectively takes a snapshot of the system, which we can evaluate for
correctness.  This evaluation proceeds by checking whether the frame
being displayed at a certain time corresponds to one of the valid
frames at that time, on any arbitrary level.  (see figure ???).  We allow
any arbitrary level because the semantic compression algorithm ensures
that all frames at a certain time will contain the same semantic
information if the semantic windows overlap.  We score the system by
summing the number of clients not showing an acceptable frame and
normalizing over the total number of clients.  A score of 0 indicates
a synchronized system.

The results for the synchronization experiments show a total score of
0 for all trials, thus no frames were missed.  This result
demonstrates that our choices for the baseline client quality levels
and the throttled bandwidths do not push the clients beyond their
bandwidth resource capacities.

% don't know if this should stay in.
% i got these numbers by manually inspecting the files where the
% clients were playing at a higher level then their alloted
% bandwidth resources and dividing the missed frames by the number
% of frames that are supposed to be shown the 5 minutes (=14)

%% To show the effect of a casual choice choice of playing levels, we
%% also ran the clients with the same bandwidth resources as above but at
%% an arbitrary quality level.  We chose the median quality level as the
%% video quality playing level and found that all clients with lower
%% bandwidth resources missed an average of 63\% of the needed frames.
%% Clients with resources equal to or above the required bandwidth for
%% the median quality level did not miss any frames.  In contrast, when
%% ran with the autonomic controller, the same clients that missed the
%% large proportion of the frames only missed 35\% of the needed frames.

\textit{Evaluating Quality of Service} \\
% since the semantic compression algorithm ensures that semantic information is preserved.
A major component of ai2tv is the autonomic controller component whose
purpose is to increase the video quality for the clients.  With
respect to the evaluation of video quality of services Liu describes
many local metrics such as frame rate, loss rate, delay jitter, image
resolution, and human spatial-temporal contrast-sensitivity
\cite{LIU}.  We do not address global metrics such as fairness, as
described in \cite{LIU}.  For our situation, a higher video quality
means a higher frame rate.

To evaluate the autonomic controller's ability to adjusts the clients,
we must define a baseline client from which to compare.  To define the
baseline client we use a value that we identify as the average
bandwidth per level.  This value is computed by summing the total size
of the frames for a certain level and dividing by the total video
time.  This value provides the bandwidth needed on average for the
buffer controller to download the next frame on time.  We use a
bandwidth throttling mechanism \cite{SHAPERD} at the server to dictate
the bandwidth available to that client.

To attain a quantitative measure of the quality of service provided by
a client assisted by the autonomic controller, we use a scoring system
relative to the baseline client's quality level.  We give a weighted
score for each level above or below the baseline quality level.  The
weighted score is calculated as the ratio of the number of frames per
minute (fpm) of the two levels.  So, for example, if a client is able
to play at one level higher then the baseline, and the baseline plays
at an average 5 fpm while the level higher plays at 10 fpm, the given
score for playing at the higher level is 2.  The weighted score is
calculated between the computed average frame rates of the chosen
quality levels.  Theoretically, the baseline client should receive a
score of 1 if that client does not miss any frames.  Note that we
formulated this scoring system because other scoring systems
\cite{scoring systems} either measure other factors such as image
resolution or human perceived quality, and are not restricted by the
group synchronization requirement.  This restriction mandates that a
scoring system be sensitive to the relative differences between
quality hierarchies.

% qos results
The evaluation of the quality of service experiments show the baseline
clients scored a group score of 1 while the clients assisted by the
autonomic controller scored a group score of 1.25.  The one-tailed
t-score of this difference is 3.01 which is significant for an
$\alpha$ value of .005 (N=17).  This result demonstrates that using
the autonomic controller, we are able to achieve a significant
positive difference in the quality of services.  Note that the t-score
does not measure the degree of the positive difference achieved by the
autonomic controller.  To demonstrate the degree of benefit of using
the autonomic controller, we measure the proportion of additional
frames that each client maintained by the controller is able to enjoy.
We found that overall, those clients received 20.4\% ($\pm$ 9.7) more
frames then the clients operating at a baseline rate.

% risk assessment
The act of running the client close to or at a level higher than the
average bandwidth needed puts the client at risk for missing more
frames because the autonomic controller is trying to push the client to
the best level.  To measure whether the controller assisted client is
exposed to a higher risk of missing frames we also count the number of
missed frames during a video session.  The scoring of the missed frame
is a simple count of the missed frames.  Note that the scoring of the
missed frame is kept separate from the measure of the relative quality
to discriminate between levels of concern, though they both indicate a
characteristic of quality of service.

%% To assess the quality of service the autonomic controller confers we
%% evaluate the log files using this scoring scheme for a selected set of
%% client configurations, specifically, 1, 2, 3, and 5 clients running a
%% video for 5 minutes and probing system state every 5 seconds.  We run
%% the system once without the autonomic controller, and then again with
%% the controller engaged.  Again we sum and normalize the score for all
%% clients and use a one-tailed t-test is used to measure the
%% significance of any difference found.

In the assessment of the risk of optimizing the frame rate we found
that there was only one instance in which the controller assisted
client missed two consecutive frames.  Upon closer inspection, the
time region during this event showed that the video demanded a higher
frame rate while the network bandwidth was relatively low.

% calculation used for the 20% number I got up there.
% baselineFrames = number of frames base client gets
% wfFrames = number of frames the wf client gets
% (wfFrames - baselineFrames) / baselineFrames = proportion of frames higher
%                                                then the baseline client

%% Sanity checks to measure the correctness of the synchronization was
%% done through visual checks of the video frames to see if they were
%% indeed showing the purported frame.

\section{Discussion} \label{discussion}
%% ??? don't know if we should bring this up.  need to discuss with
%% hellerstein about this.  
%%
%% Liu et al. also discusses convergence..., need to look up:
%% RLM and RLC (examples using this metric)

The main goals of ai2tv are distributed client video synchrony while
providing quality video.  We induced the latency delays by a per
client throttling of bandwidth at the server.  We show in our
evaluation of synchrony that, if the client chooses a quality level
appropriate to its bandwidth resources, it is able to sufficiently
maintain synchrony for multiple clients.

Though in some cases, using this system without the autonomic
controller may be sufficient, but in most cases the network bandwidth
may vary and the variable frame rate of the video do not permit the
client to make an informed decision about the most appropriate quality
level for the next frames.  In addition, an application that does not
adjust its quality level to current bandwidth resources will not be
able to offer a level of quality appropriate to the client's
resources.  To address these issues, the autonomic controller provides
an additional adaptive element to the clients.  We show that the
autonomic controller makes a significant positive difference in aiding
the client in achieving a higher quality level.

Liu et al. provide a comprehensive summary of the mechanisms used in
video multicast for quality and fairness adaptation as well as network
and coding requirements \cite{LIU}.  To frame our work in that
context, our current design models a single-rate server adaptation
scheme to each of the clients because the video quality we provide is
tailored specifically to that client's network resources.  The focus
in our work is directed towards the client side end user perceived
quality and synchrony, so we did not utilize the most efficient server
model.  The authors believe that it would be trivial to substitute in
a simulcast server adaptation model \cite{simulcast}.  Our design also
fits into the category of layered adaptation.  This adaptation model
defines a base quality level that users must achieve.  Once users have
acquired that level, the algorithm attempts to incrementally acquire
more frames to present a higher quality video.  In the work presented
here, the definition of quality translates to a higher frame rate.
Liu's discussion of bandwidth fairness, coding techniques and network
transport perspectives lie out of the scope of this paper.

\section{Related Work} \label{related}

A preliminary design and implementation of these project goals was
completed in VECTORS \cite{VECTORS}.  This system uses a peer-to-peer
synchronization scheme, a technique often employed in gaming systems,
and is tightly coupled with a collaborative virtual environment (CVE)
called CHIME \cite{CHIME}.  Due to the decentralized synchronization
scheme employed in this design and because of the heavy weight nature
of the CVE, the system failed to provide an adequate level of
synchronization needed for collaborative video viewing.

% ----------------------------
% education related support:
% ----------------------------
Other recent support for online educational support are by Guerri et
al who present a real-time e-learning system based on global satellite
for data transfer from teacher to students, and JMF and Windows Media
\cite{GUERRI} to support the distribution and display of the video
content.  Their goal is simliar to ours because their work also
attempts to make education accessible to distanced communities.  Liu
presents the the WSML system \cite{LIU2}.  This system is a web-based
synchronization multimedia lecture system for the production of a
multimedia lecture.  The synchronization referred to in this context
is that of intermedia communication to provide a replicable playback
environment.

% -------------------------------
% multicast to several clients same video content
% -------------------------------
Similar to our ability to show the same content on multiple displays,
Choi et al. show a synchronization method for real time surround
display using clustered systems \cite{CHOI} but this system is
drastically different from ours because they assume a large
communication bandwidth and homogenous end systems.

% ----------------------------
% video quality of services
% ----------------------------
Another area of related work is the use of multi-level video summaries
and their traversal in the adaptation of the video stream to provide
video quality of services
\cite{CUI,KRASIC,LEI,NEUMANN,SHIPMAN,TAN,THAKUR}.  These systems don't
provide multiple client synchronized action control, but the
contributions made in these works could be used to optimize the
content distribution server.

% ----------------------------
%% do we need to have related work wrt to the semantic compression stuff, or
%% is that out of the scope of the paper?
%
%% other semantic compression algs: can we simply say that our choice is arbitrary?
%% - phung: High level segmentation of instructional videos based on content density 
%% - Wang: Learning-based linguistic indexing of pictures with 2--d MHMMs
%% - Tarel: On the choice of similarity measures for image retrieval by example
%% - Jing: An effective region-based image retrieval framework
%% - Goh: DynDex: a dynamic and non-metric space indexer

\section{Conclusion}

In this paper we present a novel educational learning tool to allow
geographically dispersed participants to collaboratively view a video
in synchrony.  The system also employs a autonomic controller
architecture that adapts the video quality according to client network
bandwidth resources.  The other novel concept that we put forth is the
use of semantic compression to facillitate the synchronization of
video content to clients with heterogenous resources.  We rely on the
semantic compression algorithm to guarantee the semantic composition
of the video frames is similar for all clients.  We then distribute
appropriate versions of the video to clients according to their
current bandwidth resources.  Through the use of these tools, we hope
to close the gap between students with varying network resources to
allow collaboration to proceed in a fruitful manner.



%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
Professor Kender and Tiecheng Liu at the High-Level Vision Lab.
Matias Pelenur also helped a whole damn lot!!!

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ai2tv}
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

% ??? we'll need to do this right before submission
% \subsection{References}
% 
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.

% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.

\balancecolumns % GM July 2000
% That's all folks!
\end{document}
% ---------------------------------------------------------------
% / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
% / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
% / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
% ---------------------------------------------------------------