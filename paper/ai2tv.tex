% $\mathrm{AI}^2$TV
% This is "sig-alternate.tex" V1.3 OCTOBER 2002
% This file should be compiled with V1.6 of "sig-alternate.cls" OCTOBER 2002
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V1.6 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ---------------------------------------------------------------------------
% This .tex file (and associated .cls V1.6) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 2002) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2003} will cause 2002 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the 
%  copyright line.
%
% ---------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.3 - OCTOBER 2002
\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ACM-MM 2004}{New York, NY USA}
%\CopyrightYear{2001} 
% Allows default copyright year (2000) to be over-ridden - IF NEED BE.

%\crdata{0-12345-67-8/90/01}  
% Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Using Workflow to Optimize QoS for Collaborative Client Video
Synchronization}
%% \title{Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
%% Format\titlenote{(Produces the permission block, and
%% copyright information). For use with
%% SIG-ALTERNATE.CLS. Supported by ACM.}}
%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{4}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Dan Phung\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{phung@cs.columbia.edu}
\alignauthor Giuseppe Valetto\\
       \affaddr{Telecom Italia Lab}\\
       \email{valetto@cs.columbia.edu}
\alignauthor Gail Kaiser \\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{kaiser@cs.columbia.edu}
}
\additionalauthors{Additional authors: Suhit Gupta {\texttt{suhit@columbia.cs.edu}}}
\date{\parbox[b][0ex]{0em}{\hspace*{-12.5em}\raisebox{37ex}{\fbox{For
submission to \emph{ACM-MM 2004}, due 12:00 AM EDT: April 05, 2004.}}}}
% \date{05 April 2004}
\maketitle

%% groups. Participants in these groups usually have a disparity
%% in network resources which can hamper productive dialog.  We 
\begin{abstract}
The increasing popularity of distance learning and online courses has
highlighted the lack of support for collaborative tools available for
student groups. We present a design and implementation of
$\mathrm{AI}^2$TV, a system that allows geographically dispersed
participants to collaboratively view a video in synchrony.  In
addition, this system uses an autonomic workflow controller that
responds to client feedback and adjusts the quality of the video
according to the resources of each client.  We show that our system
can successfully synchronize video for distributed clients and
optimize the video quality by adaptively adjusting the frame rate. %
each client's quality of services.
\end{abstract}

% A category with the (minimum) three required fields
\category{K.3.1}{Computer Uses In Education}{Collaborative learning}
\category{H.3.4}{Systems and Software}{Distributed systems}

\terms{???}

\keywords{Synchronized Collaborative Video, Autonomic Workflow}

% tech report number CUCS-009-04

% PAPER OUTLINE
% motivation:
% - online courses becoming more popular
% - limited support for distributed collaboration
% - network resource disparity
% -> thus, we give you ai2tv
%
% INTRO
% - show trend in online courses and current state on collaborative support
% - motivate the use of video as distributed collaboration tool
% - discuss briefly history of ai2tv design and imp
% - present rest of paper and summarize achievements
% 
% BACKGROUND
% - discuss other projects on multiple client synchronization
% - ai2tv project goals
% - describe overview of semantic compression tool used
% - introduce role of process workflow
% - explain time based synchronization
% 
% DESIGN AND IMPLEMENTATION
% - design motivations of ai2tv
% - workflow controller
% 
% EVALUATION methods
% - how to score synchronized state
% - define quality in terms of better frame rate
% - explain scoring system: decouple baseline evaluation and 
% penalties missed frames.
% 
% RESULTS
% - showed ai2tv w/ and w/o workflow each compares: 
%                : one graph for baseline comparison
%                : one graph for penalties
% 1) synchronization was successful
% 2) using workflow, QoS could be significantly increased
% 
% RELATED WORK
% - what are the other available online collaborative tools?
% - what are other methods for multiple client synchronization
% 
% CONCLUSION
% - our system can be used as an aid to support collaborative
% video viewing for students groups.  

% (does this need a name? SYCOVI (SYnchronized COllaborative Video?)

% final read = check for consistent tense

\section{Introduction}
A major shift in educational paradigms is the use of online courses,
which have become increasingly more popular \cite{BELLER,DOE}.  Due to
the ease in scheduling alongside a work schedule, one of the common
type of students of online courses are the non-traditional working
student that is trying to further her or his career through higher
certifications \cite{BURGESS:TRENDS}.  This student population usually
varies geographically because online courses allow students from any
region to enroll.  The dispersed nature of the class prevents students
from physically holding study sessions, which is a common and
productive practice.  Support for collaboration is a major concern in
courses where group work is encouraged \cite{WELLS:COLLAB} yet there
are few tools that support distributed collaboration
\cite{BURGESS:TRENDS}.

The area of collaboration that our contributions apply to is
collaborative video viewing.  In this scenario, students that are
dispersed across different geographical regions can view lecture
videos simultaneously.  One proposed use of this technology is to
facilitate group study sessions to review lecture material.  While
within a video viewing session, participants can play, pause, stop and
goto certain frames synchronously as a group.

% - motivate the use of video as distributed collaboration tool
Using video on the internet requires relatively high bandiwdth
resources.  Network traffic is usually erratic, which can lead to lost
video content.  The lost content can either be good or bad, depending
on the redundancy of video material.  To eliminate this uncertainty,
we use a semantic compression package \cite{TIECHENG} to extract the
semantically rich material.  Using a selected time window, the package
summarizes the content over that time and extracts semantically rich
keyframes.  We discuss this work more extensively in section
\ref{background}.

Though all our videos are semantically rich, the playback may seem
unnatural because only keyframes are shown.  Whether the video plays
more natural depends on the number of keyframes extracted, which is a
function of the time window length.  In order to provide the best
quality video to clients, we adjust the semantic compression algoritm
to create more or less keyframes depending on the current
bandwidth\footnote{for performance reasons, we actually precompute the
keyframes for several bandwidth levels}.

% - briefly discuss ai2tv design and imp
In this paper we present our design and implementation of
$\mathrm{AI}^2$TV, which uses a time based synchronization scheme, a
common event bus and an autonomic workflow controller to respond to
feedback from each of the clients and fine tune them when possible.
We show that using these improvements we were able to synchronize
across all clients and provide an improved quality of services to each
of the the clients.

% - present rest of paper and summarize achievements
In the rest of this paper, we present the background of previous
research that supports our work.  We then present the design and
implementation of $\mathrm{AI}^2$TV including the workflow component
We then present the experimental design and results of the evaluation
of our system, and then we finish with related work and a conclusion.

\section{Background} \label{background}
% \section{The {\secit Body} of The Paper}
% - discuss other projects on multiple client synchronization
The advent of the internet and the world wide web as a major means of
communication has presented the educational community with the
potential for great gains in the realm of accessibility of education.
Some applications that support online educational collaboration are
instant message applications, application and desktop sharing (WebEx,
VNC), cobrowsing \cite{CAPPS, LIEBERMAN, SIDLER}, and XXX.  These
applications support collaboration through communicative means
(???need better phrase), but lack support of multiplexing the actual
material.  (basically I'm trying to say that none of these other apps
support the sharing of the actual educational medium: the video ).
Easy Teach and Learn proposes a virtual classroom architecture that
allows multiuser synchronous participation, but only a simulated
version of this system has been implemented \cite{WALTER}

% - ai2tv project goals
The work presented in this paper is a part of a larger project called
$\mathrm{AI}^2$TV (Adaptive Internet Interactive Team Video), which is a
multi-group collaborative project aimed at developing a collaborative
virtual environment for distributed team work.  The goal of $\mathrm{AI}^2$TV is
to provide networks of multimedia content relevant to the work carried
out by team projects, such as audio/video recordings of group
discussions and decisions, and informational and educational events.
$\mathrm{AI}^2$TV differs from many existing infrastructures that provide
educational content in a networked context by its explicit focus on
the teamwork aspects of on-line education.

%% These participants may have varying bandwidth
%% resources from a 56k modem to broadband DSL (usually 1.5 Mbps download
%% bandwidth) or cable internet (usually 30 Mbps download bandwidth).

One of the requirements for $\mathrm{AI}^2$TV is the support for the
synchronized viewing of streaming video by all members of a
geographically distributed team.  For collaboration to be effective,
all team members must be viewing the same content at all times so that
discussion will be coherent.  The possible discrepancies in each
member's bandwidth resources may leave those with lower resources
unable to participate because frames are being dropped.  To address
this issue, we employ a semantic summarization tool \cite{TIECHENG}
that reduces a video to a few semantically rich keyframes.

(???FIGURE: semantic compression )
(FIGURE: keyframes hierarchy )

% - describe overview of semantic compression tool used
The semantic summarization algorithm profiles video frames within a
given time window and selects keyframes that have the most semantic
information.  By increasing the size of the window, a keyframe will
represent a larger time slice, which means that a larger window size
will produce less keyframes as compared to a smaller window size
setting.  Thus we use windows size as an indicator for the level of
semantic compression.  Semantic summarization is used to create
multiple versions of a video by specifying different levels of
semantic compression.  Note that at each level, even though the number
of keyframes may differ, no semantic content is lost due to the nature
of the semantic summarization algorithm.  To prepare a video for
collaborative viewing, we pre-compute several sets of keyframes for
different levels.

For our purposes, we define an $\mathrm{AI}^2$TV video as the
compilation of keyframes produced from several different levels of
semantic compression.  We use a keyframe index file to access a
certain compression level of the $\mathrm{AI}^2$TV video.  Through the
use of the $\mathrm{AI}^2$TV video we can provide semantically similar
content to several clients by reducing the number of keyframes for
those clients with lesser bandwidth.  To tune the level for each
client we use process workflow technology to dynamically adapt
$\mathrm{AI}^2$TV video level in response to each client's current
bandwidth.

% - introduce role of process workflow
In addition to adjusting client viewing levels, Workflakes
\cite{PEPPO}, the process workflow controller, monitors the system of
clients and video servers and executes workflow plans in accordance
with system policies.  The dynamic adaptation of the system is
directed at modifying a combination of server and client
configurations, data caching strategies and video management schemes,
to accommodate and harmonize varying latencies, throughputs, client
processing power, and server work loads. All of these adjustments must
be completed by Workflakes within narrow time boundaries (in the order
of seconds or less), given the soft-real time nature of the
application and the kind of adaptation that must be effected.  For the
adjustments of the workflow controller to maintain integrity across
the distributed nature of servers and clients, a synchronization
scheme must underly the system.

%% \subsection{Tables}
%% Because tables cannot be split across pages, the best
%% placement for them is typically the top of the page
%% nearest their initial cite.  To
%% ensure this proper ``floating'' placement of tables, use the
%% environment \textbf{table} to enclose the table's contents and
%% the table caption.  The contents of the table itself must go
%% in the \textbf{tabular} environment, to
%% be aligned properly in rows and columns, with the desired
%% horizontal and vertical rules.  Again, detailed instructions
%% on \textbf{tabular} material
%% is found in the \textit{\LaTeX\ User's Guide}.

%% Immediately following this sentence is the point at which
%% Table 1 is included in the input file; compare the
%% placement of the table here with the table in the printed
%% dvi output of this document.

%% \begin{table}
%% \centering
%% \caption{Frequency of Special Characters}
%% \begin{tabular}{|c|c|l|} \hline
%% Non-English or Math&Frequency&Comments\\ \hline
%% \O & 1 in 1,000& For Swedish names\\ \hline
%% $\pi$ & 1 in 5& Common in math\\ \hline
%% \$ & 4 in 5 & Used in business\\ \hline
%% $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%% \hline\end{tabular}
%% \end{table}

%% To set a wider table, which takes up the whole width of
%% the page's live area, use the environment
%% \textbf{table*} to enclose the table's contents and
%% the table caption.  As with a single-column table, this wide
%% table will ``float" to a location deemed more desirable.
%% Immediately following this sentence is the point at which
%% Table 2 is included in the input file; again, it is
%% instructive to compare the placement of the
%% table here with the table in the printed dvi
%% output of this document.


%% \begin{table*}
%% \centering
%% \caption{Some Typical Commands}
%% \begin{tabular}{|c|c|l|} \hline
%% Command&A Number&Comments\\ \hline
%% \texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
%% \texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
%% \texttt{{\char'134}table}& 300 & For tables\\ \hline
%% \texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
%% \end{table*}
%% % end the environment with {table*}, NOTE not {table}!

%% \subsection{Figures}
%% Like tables, figures cannot be split across pages; the best placement
%% for them is typically the top or the bottom of the page nearest their
%% initial cite.  To ensure this proper ``floating'' placement of
%% figures, use the environment \textbf{figure} to enclose the figure and
%% its caption.

%% This sample document contains examples of \textbf{.eps}
%% and \textbf{.ps} files to be displayable with \LaTeX.  More
%% details on each of these is found in the \textit{Author's Guide}.

%% \begin{figure}
%% \centering
%% % \epsfig{file=fly.eps}
%% \caption{A sample black and white graphic (.eps format).}
%% \end{figure}

%% \begin{figure}
%% \centering
%% % \epsfig{file=fly.eps, height=1in, width=1in}
%% \caption{A sample black and white graphic (.eps format)
%% that has been resized with the \texttt{epsfig} command.}
%% \end{figure}


%% As was the case with tables, you may want a figure
%% that spans two columns.  To do this, and still to
%% ensure proper ``floating'' placement of tables, use the environment
%% \textbf{figure*} to enclose the figure and its caption.
%% \begin{figure*}
%% \centering
%% \epsfig{file=flies.eps}
%% \caption{A sample black and white graphic (.eps format)
%% that needs to span two columns of text.}
%% \end{figure*}
%% and don't forget to end the environment with
%% {figure*}, not {figure}!

%% Note that either {\textbf{.ps}} or {\textbf{.eps}} formats are
%% used; use
%% the \texttt{{\char'134}epsfig} or \texttt{{\char'134}psfig}
%% commands as appropriate for the different file types.

%% \begin{figure}
%% \centering
%% \psfig{file=rosette.ps, height=1in, width=1in,}
%% \caption{A sample black and white graphic (.ps format) that has
%% been resized with the \texttt{psfig} command.}
%% \end{figure}

%% \subsection*{A {\secit Caveat} for the \TeX\ Expert}
%% Because you have just been given permission to
%% use the \texttt{{\char'134}newdef} command to create a
%% new form, you might think you can
%% use \TeX's \texttt{{\char'134}def} to create a
%% new command: \textit{Please refrain from doing this!}
%% Remember that your \LaTeX\ source code is primarily intended
%% to create camera-ready copy, but may be converted
%% to other forms -- e.g. HTML. If you inadvertently omit
%% some or all of the \texttt{{\char'134}def}s recompilation will
%% be, to say the least, problematic.

\section{Design} \label{design}

% design of a the system in general 
The basic design of our system involves several components: a video
server, a video client and a synchronization architecture and a
workflow controller.

The video server provides the content to the clients for viewing.  The
semantic compression package operates on MPEG format videos and
outputs JPG frames.  An $\mathrm{AI}^2$TV video is produced by running
the tool with several compression levels which produces a set JPG
frames which are indexed by a frame index file.  The video server's
task then is to simply provide remote access to these frames.

The video client's task is to acquire the video frames, display them
at the correct time, and provide basic video functions, specifically
play, pause, goto, and stop.  Given the simplistic video server model,
this task could be achieved by keeping a video clock, downloading the
frames of the video for a given quality level, and displaying a given
frame at its representative time.  Taking a functional design approch,
the client employs three controllers for time, video cache, and video
display.

The synchronization architecture provides the binding across multiple
clients that allows client actions and video content to be
synchronzied.  The architecture provides a mechanism to gauge system
synchronization and to adjust individual clients relative to each
client's state.

(FIGURE: ai2tv synchronization arch)

$\mathrm{AI}^2$TV employs a global time based synchronization
architecture that used a central event bus.  The global clock was
synchronized across all clients so each client references its own
clock to synchronize.  The central event bus is used to synchronize
video actions, such as pause and stop.  These video actions are
themselves time stamped so when clients respond to these directives,
they are still in reference to the central time.  

%% In addition to a difference in synchronization architecture, system 2
%% also contained an autonomic component that adjusted the clients in
%% response to their feedback.

The worklow system design follows that of a autonomic reference
architecture proposed by Kaiser \cite{refarch}.  (XXX figure of ref
arch here).  The flow of events in this architecture is that probes
collect data from the clients and send that data to a gauge.  The
gauge collects inputs from many probes and responds by enacting the
workflow engine to enact a workflow plan.  That plan is executed at
the client end by effectors.  We use this architecture to gauge the
resources of the client and adjust them to receive better quality of
services.

\section{Implementation} \label{implementation}

Though the designs of both systems are not drastically different other
than the synchronization architecture, the implementations of the two
systems vary drastically other than the video server, which is simply
a web server that provides access to the $\mathrm{AI}^2$TV video.

The $\mathrm{AI}^2$TV video display is implemented in Java and simply
renders the JPG frame in a window.  The video frame display controller
gets a list of available frames from the cache controller.  The
display client has a separate control for quality level that allows
the client to play at a certain quality level while the cache
controller downloads at a different level.  This feature allows the
workflow controller, described later in this section, to adjust the
client's display and cache separately and intelligently in response to
current bandwidth resources.

The cache controller is simply a downloading daemon that will continue
downloading at a certain quality level.  Directives to change the
quality level and other adjustments to the cache controller are
decided by the workflow controller.  The cache controller keeps a hash
of the available frames and a count of the current reserve frames
(frames buffered) for a given quality level.

%% and uses a thread sleep function to control the time
%% controller thread from hogging the processor.

The time keeping scheme uses the hardware clock as the base for its
internal video clock and  to ensure that the system clocks across
the $\mathrm{AI}^2$TV system are synchronized.  The separation of the
synchronization scheme greatly simplifies the rest of the system
because it clarifies the functional responsibilities of the other
components.  Each client plays the video frames at the correct time
and since all the clients have the same time, then all the clients
play in tandem.  Since the client is simply checking the time and
displaying the resepective video frame for that time, unless it is
misinformed about the correct frame to display, it will display the
correct video.  All video actions and workflow directives are
timestamped so the client can execute the actions in accordance with
that time.

The workflow controller makes decisions on behalf of the clients to
adjust their video quality levels according to collected statistics .
For each client, the workflow probes for the video display quality
level, cache controller quality level, cache controller reserve
frames, current frame and bandwidth.  The workflow directives can be
grouped into two categories: rules that adjust the client in response
to relatively low bandwidth situations and orders that take advantage
of relatively high bandwidth situations.  The gauge monitors the state
of the clients using the probe statistics described above and compares
them to configurable threshold values and mandates decisions
accordingly.

In the situation where a client has relatively low bandwidth, the
client may not download a frame in time.  This situation will merit
that both the client and the cache quality levels are reduced one
level.  In the case where the client is already at the lowest level,
the workflow controller will calculate the next possible frame that it
can successfully complete in time.

To take advantage of relatively high bandwidth situations, the cache
controller will accumulate a reserve buffer.  Once the buffer reaches
a threshold value, the workflow controller will direct the cache
controller to start downloading frames a higher quality level.  Once
the cache controller has downloaded a sufficient reserve at that
level, the client is then ordered to display the higher quality level.
Note that a higher quality level means a better frame rate, not a
better quality JPG.  If the bandwidth drops before the cache
controller can accumulate a sufficient buffer, then the cache
controller is dropped back down to the client level.  This incremental
increase in quality level prevents the client from entering a cycle
where it is repeatedly downgraded several levels due to a high
frequency change in frames.  There are pockets of high frequency
frames within the $\mathrm{AI}^2$TV video that reflects a portion of
the original video that has a large amount of semantic change.

The workflow controller allows the clients to take advantage or adjust
to relative deviations in bandwidth that occurs during a video
session.  Also, due to pockets of high frequency semantic change in
the video, the client must be carefully monitored during those times
to ensure that it doesn't drop those important frames.

\section{Evaluation Methods} \label{eval}

(FIGURE: show synchrony of how frames line up)

\textit{Evaluating Synchrony} \\ The purpose of the system is to
provide synchronous viewing to all clients.  To measure the
effectiveness of the synchrony, we probe the clients at and peroidic
time intervals and log the frame currently being displayed.  This
procedure effectively takes a snapshot of the system, which we can
evaluate for correctness.  This evaluation proceeds by checking
whether the frame being displayed at a certain time corresponds to one
of the valid frames at that time, on any arbitrary level.  (see
figure).  We allow any arbitrary level because the semantic
compression algorithm ensures that all frames at a certain time will
contain the same semantic information (???can we get kender to check
this, maybe a reference?) if the ``semantic windows'' overlap (???need
to get the correct term for this from kender).  We score the system by
summing the number of clients not showing an acceptable frame and
normalizing over the total number of clients.  A score of 0 indicates
a synchronized system.  We asses our system's using this evaluation
method on a selected set of client configurations, specifically, 1, 2,
3, 5 and 10 clients running a video for 5 minutes.  A two-tailed
t-test is used to measure the significance of any difference found.

\textit{Evaluating Quality of Service} \\
% since the semantic compression algorithm ensures that semantic information is preserved.
A major component of $\mathrm{AI}^2$TV is the workflow component whose
purpose is to increase the video quality for the clients.  For our
situation, a higher video quality means a higher frame rate.  Since we
are trying to measure how much better or worse the workflow adjusts
the clients, we must define a baseline client from which to compare.
To define the baseline client we will use a value that we identify as
the average bandwidth per level.  This value is computed by summing
the total size of the frames for a certain level and dividing by the
total video time.  This value provides the bandwidth needed on average
for the cache controller to download the next frame on time.

A baseline client also needs a context in which it is the baseline, so
we also use a bandwidth throttling mechanism \cite{shaperd} at the
server to dictate the bandwidth available to a certain client.  A
baseline client is thus defined as a client who is playing videos at
the quality level matching that of its bandwidth, which is found by
comparing the average bandwidth per level and the alloted bandwidth
and that level which has a needed bandwidth with a minimal positive
difference with the alloted bandwidth (the client just has enough
bandwidth to maitain that level).

To attain a quantitative measure of the quality of service provided by
a workflow assisted client, we use a scoring system relative to the
baseline client's quality level.  We give a score of 1 for each
quality level above the baseline quality level and -1 for each quality
level below.  Theoretically, the baseline client should receive a
score of 0.

%% The possible reasons for missing frames are an abrupt change in
%% bandwidth or a high frequency change in frames

The act of running the client close to or at a level higher than the
average bandwidth needed puts the client at risk for missing more
frames because the workflow is trying to push the client to the best
level.  To measure whether the workflow assisted client is exposed to
a higher risk of missing frames we also count the number of missed
frames during a video session.  The scoring of the missed frame is a
simple count of the missed frames.  Note that the scoring of the
missed frame is kept separate from the measure of the relative quality
to discriminate between levels of concern, though they both indicate a
characteristic of quality of service.

To assess the quality of service the workflow confers we evaluate the
log files using this scoring scheme for a selected set of client
configurations, specifically, 1, 2, 3, 5 and 10 clients running a
video for 5 minutes.  We run the system once without workflow, and
then again with the workflow engaged.  Again we sum and normalize the
score for all clients and use a two-tailed t-test is used to measure
the significance of any difference found.

%% Sanity checks to measure the correctness of the synchronization was
%% done through visual checks of the video frames to see if they were
%% indeed showing the purported frame.

\section{Results} \label{results}

\section{Discussion} \label{discussion}

\section{Related Work} \label{related}

Other work being done on CSCW schemes?  Other multiclient synchronous 
systems (MOOs and MUDs), what else?

\section{Future Work} \label{con}

Try a push model with the server.  Integrate streaming audio.

\section{Conclusions}

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}

Professor Kender and Tiecheng Liu at the High-Level Vision Lab.
Matias Pelenur also helped.


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ai2tv}
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%

\subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.

% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.

\balancecolumns % GM July 2000
% That's all folks!
\end{document}
