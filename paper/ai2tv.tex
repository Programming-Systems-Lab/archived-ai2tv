% $RCSfile$
% $Revision$
% $Date$
% $Source$
%
%
% ---------------------------------------------------------------------------
% TODO:
% - equate workflow controller +probes/gauges = feedback controller (only use this term).
% - explain early on about the characteristic of fluctuations in bandwidth and
%   high freq changes in video
%
% FINAL READ 
% - check for consistent tense
% - query replace: ai2tv -> $\mathrm{AI}^2$TV
% - remove margin hack in sig-alt.cls
% - create reference section from .bbl file
%
% ---------------------------------------------------------------------------
% This is "sig-alternate.tex" V1.3 OCTOBER 2002
% This file should be compiled with V1.6 of "sig-alternate.cls" OCTOBER 2002
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V1.6 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ---------------------------------------------------------------------------
% This .tex file (and associated .cls V1.6) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 2002) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2003} will cause 2002 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the
%  copyright line.
%
% ---------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.3 - OCTOBER 2002
\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ACM-MM 2004}{New York, NY USA}
%\CopyrightYear{2001}
% Allows default copyright year (2000) to be over-ridden - IF NEED BE.

%\crdata{0-12345-67-8/90/01}
% Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Optimizing Quality for Collaborative Multi-client Video}
%% \title{Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
%% Format\titlenote{(Produces the permission block, and
%% copyright information). For use with
%% SIG-ALTERNATE.CLS. Supported by ACM.}}
%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{4}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Dan Phung\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{phung@cs.columbia.edu}
\alignauthor Giuseppe Valetto\\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \affaddr{and Telecom Italia Lab}\\
       \affaddr{Turin, Italy}
       \email{valetto@cs.columbia.edu}
\alignauthor Gail Kaiser \\
       \affaddr{Computer Science Department}\\
       \affaddr{Columbia University}\\
       \affaddr{New York City, New York}\\
       \email{kaiser@cs.columbia.edu}
}
\additionalauthors{Additional authors: Suhit Gupta {\texttt{suhit@columbia.cs.edu}}}
\date{\parbox[b][0ex]{0em}{\hspace*{-12.5em}\raisebox{37ex}{\fbox{For
submission to \emph{ACM-MM 2004}, due 12:00 AM EDT: April 05, 2004.}}}}
% \date{05 April 2004}
\maketitle

%% groups. Participants in these groups usually have a disparity
%% in network resources which can hamper productive dialog.  We
\begin{abstract}
The increasing popularity of distance learning and online courses has
highlighted the lack of support for collaborative tools available for
student groups. We present a design and implementation of ai2tv, a
system that allows geographically dispersed participants to
collaboratively view a video in synchrony.  Video actions can be
initiated by any of the participants and the results of those actions
are seen by all the members.  In addition, this system uses an
autonomic feedback controller that responds to client feedback and
adjusts the quality of the video according to the resources of each
client.  We show that our system can successfully synchronize video
for distributed clients and optimize the video quality by adaptively
adjusting the frame rate.
\end{abstract}

% A category with the (minimum) three required fields

\category{C.2.4}[Distributed Systems]{Client/server, Distributed applications}
\category{D.2.8}[Software Engineering]{Metrics -- performance measures}
\category{H.5.1}[Information Interfaces and Presentation]{Multimedia Information Systems}
\category{H.5.3}[Group and Organization Interfaces]{Computer-supported cooperative work, Synchronous interaction}
\category{K.3.1}{Computer Uses In Education}{Collaborative learning, Distance learning}

\terms{ALGORITHMS, MEASUREMENT, PERFORMANCE, EXPERIMENTATION, HUMAN
FACTORS}

\keywords{Synchronized Collaborative Video, Autonomic Feedback Controller}

% tech report number CUCS-009-04

% PAPER OUTLINE
% motivation:
% - online courses becoming more popular
% - limited support for distributed collaboration
% - network resource disparity
% -> thus, we give you ai2tv
%
% INTRO
% - show trend in online courses and current state on collaborative support
% - motivate the use of video as distributed collaboration tool
% - discuss briefly history of ai2tv design and imp
% - present rest of paper and summarize achievements
%
% BACKGROUND
% - discuss other projects on multiple client synchronization
% - ai2tv project goals
% - describe overview of semantic compression tool used
% - introduce role of process workflow
% - explain time based synchronization
%
% DESIGN AND IMPLEMENTATION
% - design motivations of ai2tv
% - workflow controller
%
% EVALUATION methods
% - how to score synchronized state
% - define quality in terms of better frame rate
% - explain scoring system: decouple baseline evaluation and
% penalties missed frames.
%
% RESULTS
% - showed ai2tv w/ and w/o workflow each compares:
%                : one graph for baseline comparison
%                : one graph for penalties
% 1) synchronization was successful
% 2) using workflow, QoS could be significantly increased
%
% RELATED WORK
% - what are the other available online collaborative tools?
% - what are other methods for multiple client synchronization
%
% CONCLUSION
% - our system can be used as an aid to support collaborative
% video viewing for students groups.

% (does this need a name? SYCOVI (SYnchronized COllaborative Video?)

\section{Introduction}
A major shift in educational paradigms is the use of online courses,
which have become increasingly more popular \cite{BELLER,DOE}.  Due to
the ease in scheduling alongside a work schedule, one of the common
type of students of online courses are the non-traditional working
student that is trying to further her or his career through higher
certifications \cite{BURGESS}.  This student population usually
varies geographically, since online courses allow students from any
region to enroll.  The dispersed nature of the class prevents students
from physically holding group study sessions, which is commonly
regarded by students as a useful and productive practice.  Support for
collaboration is a major concern in courses where group work is
encouraged \cite{WELLS}, yet there are few tools that support
distributed collaboration \cite{BURGESS}.

The area of collaboration that our contribution applies is
collaborative video viewing.  In this scenario, students that are
dispersed across different geographical regions can view lecture
videos simultaneously and in a synchronized manner: Within a video
session, participants can play, pause, stop and go to certain frames
synchronously as a group.  Any member of the video session can
initiate the actions, as opposed to previous action models where only
one user has the controller and designates the flow of the video (???
cite master/slave controller systems).  The intent is to facilitate
the review of lecture material during distributed collaborative study
sessions, in order, for instance, to solve some difficulty in a joint
project assignment, or to clarify some misconceived lecture material.

% - motivate the use of video as distributed collaboration tool
Viewing video on the Internet requires relatively high bandiwdth
resources.  Network traffic is usually erratic, which can lead to lost
video content. Depending on the redundancy of video material, the lost
content may or may not contain significant information.  To eliminate
this uncertainty, we use a semantic compression package
\cite{TIECHENG} to extract relevant material and produce semantically
rich videos.  The package summarizes the content within a shifting
time window by extracting semantically rich key frames.  The resulting
key frames compose the video that is streamed to the clients.  We
discuss this process more extensively in section \ref{background}.

% introduce the discussion on video quality.
A second but important focus in our project is providing good quality
video to the clients.  Frame rate is commonly used as an indicator to
evaluate quality of video (citations): higher being better.  Because
the video from the semantic compression scheme produces randomly
dispersed key frames, video playback is at a slower variable frame
rate, which may seem unnatural to the viewer.  In order to provide an
appropriate viewing experience to different clients, whom may have
varying bandwidth and computer resources, we construct several
different versions of the same video content and use a feedback
controller to adjust the viewing level per client's resources.

% - briefly discuss ai2tv design and imp
In this paper we present our design and implementation of a
collaborative video viewing tool, developed in the context of a larger
project, called ai2tv (Adaptive Internet Interactive Team
Video).  Our tool employs semantic compression, a time based
synchronization scheme, a common event bus and an autonomic
workflow-based controller to respond to feedback from the clients and
fine tune their viewing experience.  We show that using these
mechanisms we are able to synchronize content viewing across all
clients while ensuring an improved video quality to each of the the
clients.

% - present rest of paper and summarize achievements
In the rest of this paper, we present the background of previous
research that supports our work.  Next we present the design and
implementation of ai2tv.  We then present the experimental
design and results, and we finish with related work and a discussion
of our conclusions.


\section{Background} \label{background}
% - discuss other projects on multiple client synchronization
The advent of the Internet and the World Wide Web as a major means of
communication has presented the educational community with the
potential for great gains in the realm of accessibility to education
through eLearning.  Among applications that support online educational
collaboration (although they may not be geared specifically towards
educational purposes), there are instant message applications,
application and desktop sharing (WebEx, VNC), co-browsing \cite{CAPPS,
LIEBERMAN, SIDLER}, and XXX.  These applications, for the most part,
facilitate the communicative aspects of collaboration such as the
supporting long-distance exchange of ideas and know-how (peppo, what
do you mean by know-how?) among collaborators.  However, there is a
dearth of tools to support the collaborative review and use of
educational material produced for on-line courses (cite JITE paper).
Most eLearning applications follow the metaphor of a Web portal where
registered users can individually access multimedia class material or
post completed assignments.  Only a few applications support
cooperation for on-line study environments: for example, Easy Teach
and Learn proposes a virtual classroom architecture that allows
multiuser synchronous participation, but only a simulated version of
this system has been implemented \cite{WALTER}

% - ai2tv project goals
The work presented in this paper is a part of the ai2tv
project, a multi-group effort aimed at developing a collaborative
virtual environment for distributed team work.  The goal of
ai2tv is to provide networks of multimedia content
relevant to the work carried out by teams of users involved in the
same project, such as audio/video recordings of group discussions and
decisions, and informational and educational events.
ai2tv differs from many existing infrastructures that
provide educational content in a networked context by its explicit
focus on the teamwork aspects of on-line education.

% VECTORS (sys 1) and CHIME are mentioned in related work.

%% These participants may have varying bandwidth
%% resources from a 56k modem to broadband DSL (usually 1.5 Mbps download
%% bandwidth) or cable internet (usually 30 Mbps download bandwidth).

One of the requirements of the ai2tv multimedia subsystem
is the support for the synchronized viewing of streaming video by all
members of a geographically distributed team. To enforce the metaphor
of synchronous collaboration in the virtual environment, and for that
collaboration to be effective, all team members must be viewing the
same content at all times so that discussion of the material can be
coherent.  Synchronized viewing, however, is particularly difficult in
a context in which the multimedia content is to be delivered over
heterogeneous Internet links to heterogeneous platforms: team members
can be dispersed over the Internet, and may enjoy very diverse
connectivity, ranging for example from 56k modem, to DSL, to cable, to
T1 lines.  Moreover, in such a setup, the communication and computing
resources available to each user may widely and quickly vary in the
course of the team work session.

(???FIGURE: semantic compression )
(???FIGURE: key frames hierarchy )

%% Thus, we use the window size as an indicator for the level of semantic
%% compression.

% - describe overview of semantic compression tool used
To address those issues and achieve synchronized viewing, we employ a
semantic summarization tool \cite{TIECHENG} that reduces a video to a
set of semantically significant key frames.  This package operates on
MPEG format videos and outputs sequences of JPG frames.  The semantic
compression algorithm profiles video frames within a sliding time
window and selects key frames that have the most semantic information.
By increasing the size of the window, a key frame will represent a
larger time slice, which means that a larger window size will produce
less key frames as compared to a smaller window size setting.
Semantic summarization is used to create multiple versions of a video
by specifying different levels of semantic compression.  Note that at
each level, even though the number of key frames may differ, no
semantic content is lost due to the nature of the semantic
summarization algorithm.  To prepare a video for collaborative
viewing, we pre-compute several sets of key frames, obtaining a
hierarchy of semantic compression levels.

For our purposes, we define an ai2tv video as the
compilation of key frames produced from several different levels of
semantic compression.  We use a key frame index file to access a
certain compression level of the ai2tv video.  Through the
use of the ai2tv video we can provide semantically similar
content to several clients with diverse resources by adjusting a
client's compression level.  To tune the level for each client we use
a feedback controller to dynamically adapt the client's video viewing
level in response to each client's current bandwidth.

Due to the nature of the semantic compression algorithm and the random
dispersion of the key frames the resultant ai2tv video stream plays at
a variable frame rate.  Within the video stream there are pockets of
relatively high frequency semantic change that result in a high demand
frame rate during that time.  The client must be able to download and
display all the video frames consistently in an environment of
possibly erratic network bandwidth.  To aid the client in this task,
the autonomic controller monitors the client's state and resources and
adapts the client accordingly.

% - introduce role of autonomic controller
The autonomic controller monitors the distributed ai2tv system
composed of clients and video servers, analyzes incoming streams of
sensor data, and employs a workflow engine, named Workflakes
\cite{PEPPO}, to execute as needed workflow processes that effect in a
coordinated fashion the dynamic adaptation of the video viewing
settings across a group of clients.  That dynamic adaptation aims at
maintaining the clients in the same group synchronized, in accordance
with policies that implement an underlying distributed synchronization
scheme, while at the same time fine tuning the quality of the viewing
experience for individual clients.

\section{Design} \label{design}

% Design of a the system in general
The design of our system involves several major components: a video
server, video clients, and an externalized autonomic controller.

%
%(FIGURE: ai2tv synchronization arch)

% video server
The video server provides the multimedia educational content to the
clients for viewing.  The provided content, an ai2tv video, is
produced by running the tool multiple times with settings for
different compression levels, which produces several sets of JPG
frames that are indexed by a frame index file.  The task of the video
server is to simply provide remote download access to these frames.

% video client
The task of video clients is to acquire video frames, display them at
the correct time, and provide a set of basic video functions,
specifically play, pause, goto, and stop.  Given the simple video
server model, this task can be achieved by keeping a video clock,
downloading the frames of the video for a given quality level, and
displaying a given frame at its representative time.  The clocks of
all clients are kept in sync using NTP \cite{NTP} so that each client
can use its local clock with a guarantee that it refers to a common
time base. Taking a functional design perspective, the client is thus
composed of three major modules: the time controller, video cache
controller, and video display. The role of the cache controller is
particularly important, since it is in charge of acquiring the content
to be shown and it is the locus where adaptation takes place in the
client.

% autonomic controller
The externalized autonomic controller provides the mechanisms that
allow client actions and video content to be synchronized across
multiple clients. The controller is itself a distributed system, whose
design derives from a conceptual reference architecture for autonomic
computing platforms proposed by Kaiser \cite{refarch}.  (??? figure of
ref arch here). According to that architecture, sensors attached to
elements of the target system of the autonomic platform continuously
collect data and send it to gauges.  The role of the gauges is to
analyze the data flow from multiple sensors and deterine whether some
adaptation is needed by the target system.  If so, they send a trigger
to the controller's core, a coordination component that is in charge
of enacting a set of corresponding changes onto (possibly) multiple
components of the target system.  To achieve that, this coordinator
orchestrates, according to a defined plan, the work of a cohort of
computations (referred to as effectors), which are executed at the
target system.  That coordination plan may need to account for complex
and dynamic sequencing and logic dependencies among the various units
of work represented by effectors' executions and may need to offer
provisions for contingency planning and compensations. The conceptual
architecture does not indicate or mandate a preferred approach to
building the coordinator in a way that complies with those
requirements.  However, a valid option to specify and enact the
coordination plans - and the one chosen in the context of this work -
is offered by workflow technology.

% should this stuff go to the BG or Discussion section? 
This autonomic architecture effectively provides a closed control loop
that can be superimposed on a target system that does not have
built-in autonomic features.  It can take a reactive stance
(detect-and respond), resulting in a feedback loop, and also a
proactive stance (detect-and-anticipate), resulting in a feed-forward
loop.  The architecture remains also largely orthogonal and disjoint
from the target system and its own specifics, thus representing a
general-purpose tool that can be employed independently from the
application domain. (???we should cite some other computing feedback
controller system (IBM's lotus notes/apache stuff)

% 999 ------------------------- CURRENT ------------------------- %

% communciation bus and sensors
Communication within the distributed system is provided by an event
communication bus based on the publish/subscribe paradigm.  The reason
for choosing this communication model is that it inherently decouples
the physical location of the communicating entities.  Events
transmitted onto the event bus include video actions, such as pause
and stop, which are time stamped so that clients can respond to these
directives in reference to the common time base.  Embedded in each
client are sensors that periodically publish events to the autonomic
controller to allow it to monitor the state and resources of each
client.

%% PEPPO - NOTE: here I would show an example of those events, i.e. its
%% content in terms pof attribute/value pairs with a bit of explanation.
%% (dan suggests you move this example to the implementation section)
%
Gauges receive the events sent by the sensors and evaluate the
client's ability to fetch and display the correct content in time,
respective of some compression level in the hierarchy provided by the
ai2tv.  These gauges are embedded together with the Workflakes
coordinator for expediency of design and to minimize the communication
latency between the gauges and Workflakes when adaptation is needed.
Gauges use a set of helper functions that implement the ai2tv
synchronization scheme, to evaluate periodically (e.g. each second) if
a group of clients is in sync at that time, and, if not, to trigger
appropriate adaptation directives.  These directives, which are
executed under the orchestration of Workflakes, are sent in the form
of events onto the bus and are received only by the impacted clients.
Effectors respond to incoming adaptation events by executing the
directives, such as changing the quality level or modifying the cache
fetching level.

%% we actually have sensors and effectors in the display module
%% as well.  though most of the ``intelligent'' adjustment happens in
%% the cache controller, the sensor and effector act on the client display
%% and cache controller.
% 
%% Notice how, by having sensors and effectors placed onto the cache of
%% the clients (as opposed to the display module), the autonomic
%% controller deals with video content that is about to be displayed
%% (rather than being currently displayed). That way, it puts in place a
%% detect-and-anticipate control loop, which takes preemptive rather than
%% reactive action; that is very important for our purposes, since we
%% want to avoid as much as possible situations that can lead to
%% synchronization faults, rather than recovering from them.

\section{Implementation} \label{implementation}

The ai2tv video display is implemented in Java and simply renders the
JPG frame in a window.  The video frame display controller gets a list
of available frames from the cache controller.  The display client has
a separate control for quality level that allows the client to play at
a certain quality level while the cache controller downloads at a
different level.  This feature allows the feedback controller,
described later in this section, to adjust the client's display and
cache separately and intelligently in response to current bandwidth
resources.

The cache controller is simply a downloading daemon that will continue
downloading at a certain quality level.  Directives to change the
quality level and other adjustments to the cache controller are
decided by the feedback controller.  The cache controller keeps a hash
of the available frames and a count of the current reserve frames
(frames buffered) for a given quality level.

The time keeping scheme uses the hardware clock as the base for its
internal video clock and to ensure that the system clocks across the
ai2tv system are synchronized.  The separation of the synchronization
scheme greatly simplifies the rest of the system because it clarifies
the functional responsibilities of the other components.  Each client
plays the video frames at the correct time and since all the clients
have the same time, then all the clients play in tandem.  Since the
client is simply checking the time and displaying the resepective
video frame for that time, unless it is misinformed about the correct
frame to display, it will display the correct video.  All video
actions and controller directives are timestamped so the client can
execute the actions in accordance with that time.

We chose Siena as our publish-subscribe event system for its ability
to scale.  Siena is a content-based networking communication
infrastructure that facillitates the passing of messages by routing
events by content \cite{SIENA}.  This communication paradigm removes
the need to introduce explicit routing mechanisms within our system
and its simplistic design provides high performance results.  We
defined two basic types of messages, one type for video actions and
another type for feedback controller related activities.  

The video action events include the video action type (play, pause,
stop, goto), the unique video session identifier, and a timestamp.
All clients within the same video session will receive that message
and execute the appropriate action.  Note that the client that
initiates the action only executes the action upon receipt of the
event so that the initiating client does not become out of sync with
the other clients by having early knowledge of the video action.

There are two feedback controller related messages: client state
events sent out by the embedded probes and controller directives that
adjust the clients.  We elaborate the contents of these messages as we
describe the feedback controller below.

The feedback controller makes decisions on behalf of the clients to
adjust their video quality levels according to collected statistics .
For each client, the feedback controller probes for the video display
quality level, cache controller quality level, cache controller
reserve frames, current frame and bandwidth.  The feedback controller
directives can be grouped into two categories: rules that adjust the
client in response to relatively low bandwidth situations and orders
that take advantage of relatively high bandwidth situations.  The
contents of these directives include the unique client identifier, and
a directive.  The directives can adjust the client's display quality
level or the client's cache quality level, or the directive can tell
the client to jump ahead to a future frame.  This last directive is
sent only in dire bandwidth situations where the controller must
calculate the next future frame that the client can possibly download
given its current bandwidth resources.  The gauge monitors the state
of the clients using the probe statistics described above and compares
them to configurable threshold values and mandates decisions
accordingly.

In the situation where a client has relatively low bandwidth, the
client may not download a frame in time.  This situation will merit
that both the client and the cache quality levels are reduced one
level.  In the case where the client is already at the lowest level,
the feedback controller will calculate the next possible frame that it
can successfully complete in time.

To take advantage of relatively high bandwidth situations, the cache
controller will accumulate a reserve buffer.  Once the buffer reaches
a threshold value, the feedback controller will direct the cache
controller to start downloading frames a higher quality level.  Once
the cache controller has downloaded a sufficient reserve at that
level, the client is then ordered to display the higher quality level.
Note that a higher quality level means a better frame rate, not a
better quality JPG.  If the bandwidth drops before the cache
controller can accumulate a sufficient buffer, then the cache
controller is dropped back down to the client level.  This incremental
increase in quality level prevents the client from entering a cycle
where it is repeatedly downgraded several levels due to a high
frequency change in frames.  There are pockets of high frequency
frames within the ai2tv video that reflects a portion of the original
video that has a large amount of semantic change.

%% need more stuff about the WF, such as the lil jil stuff, cougaar, etc.
The feedback controller allows the clients to take advantage or adjust
to relative deviations in bandwidth that occurs during a video
session.  Also, due to pockets of high frequency semantic change in
the video, the client must be carefully monitored during those times
to ensure that it doesn't drop those important frames.

\section{Evaluation Methods} \label{eval}

(FIGURE: show synchrony of how frames line up)

To evalute our system, we produced an ai2tv video that had 5 quality
levels.  For a 17 minute video and five different window lengths, the
total number of frames are 165, 71, 39, 21, and 13.  Our choice of the
relatively low frame rate quality levels was influenced by the goal of
the system being used by clients with low bandwidth resources.
 
% the pathetic average frame rates (per minute!!!):
%% 3.399831413 - high
%% 1.46295776
%% 0.806289939
%% 0.434237734
%% 0.268763313 - low

\textit{Evaluating Synchrony} \\ The purpose of the system is to
provide synchronous viewing to all clients.  To measure the
effectiveness of the synchrony, we probe the clients at peroidic time
intervals and log the frame currently being displayed.  This procedure
effectively takes a snapshot of the system, which we can evaluate for
correctness.  This evaluation proceeds by checking whether the frame
being displayed at a certain time corresponds to one of the valid
frames at that time, on any arbitrary level.  (see figure).  We allow
any arbitrary level because the semantic compression algorithm ensures
that all frames at a certain time will contain the same semantic
information (???can we get kender to check this, maybe a reference?)
if the ``semantic windows'' overlap (???need to get the correct term
for this from kender).  We score the system by summing the number of
clients not showing an acceptable frame and normalizing over the total
number of clients.  A score of 0 indicates a synchronized system.  We
asses our system's using this evaluation method on a selected set of
client configurations, specifically, 1, 2, 3, and 5 clients running a
video for 5 minutes and probing system state every 5 seconds.

\textit{Evaluating Quality of Service} \\
% since the semantic compression algorithm ensures that semantic information is preserved.
A major component of ai2tv is the feedback controller component whose
purpose is to increase the video quality for the clients.  For our
situation, a higher video quality means a higher frame rate.  Since we
are trying to measure how much better or worse the feedback controller
adjusts the clients, we must define a baseline client from which to
compare.  To define the baseline client we will use a value that we
identify as the average bandwidth per level.  This value is computed
by summing the total size of the frames for a certain level and
dividing by the total video time.  This value provides the bandwidth
needed on average for the cache controller to download the next frame
on time.

Now that we know how much bandwidth a client needs on average, we can
throttle the bandwidth to that client to match that average.  This
procedure presents a baseline client model that is based on averages,
which may not be realistic but is a close approximation.  Using the
average as the baseline doesn't account for high frequency changes in
the video frames (make sure that we've discussed this before) and
intermittent fluctuations in network bandwidth.

A baseline client also needs a context in which it is the baseline, so
we also use a bandwidth throttling mechanism \cite{SHAPERD} at the
server to dictate the bandwidth available to a certain client.  A
baseline client is thus defined as a client who is playing videos at
the quality level matching that of its bandwidth, which is found by
comparing the average bandwidth per level and the alloted bandwidth
and that level which has a needed bandwidth with a minimal positive
difference with the alloted bandwidth (the client just has enough
bandwidth to maitain that level).

To attain a quantitative measure of the quality of service provided by
a client assisted by the feedback controller, we use a scoring system
relative to the baseline client's quality level.  We give a weighted
score for each level above or below the baseline quality level.  The
weighted score is calculated as the ratio of the number of frames per
minute (fpm) of the two levels.  So, for example, if a client is able
to play at one level higher then the baseline, and the baseline plays
at an average 5 fpm while the level higher plays at 10 fpm, the given
score for playing at the higher level is 2.  The weighted score is
calculated between the computed average frame rates of the chosen
quality levels.  Theoretically, the baseline client should receive a
score of 1 if that client does not miss any frames.

The act of running the client close to or at a level higher than the
average bandwidth needed puts the client at risk for missing more
frames because the feedback controller is trying to push the client to
the best level.  To measure whether the controller assisted client is
exposed to a higher risk of missing frames we also count the number of
missed frames during a video session.  The scoring of the missed frame
is a simple count of the missed frames.  Note that the scoring of the
missed frame is kept separate from the measure of the relative quality
to discriminate between levels of concern, though they both indicate a
characteristic of quality of service.

To assess the quality of service the feedback controller confers we
evaluate the log files using this scoring scheme for a selected set of
client configurations, specifically, 1, 2, 3, and 5 clients running a
video for 5 minutes and probing system state every 5 seconds.  We run
the system once without the feedback controller, and then again with
the controller engaged.  Again we sum and normalize the score for all
clients and use a one-tailed t-test is used to measure the
significance of any difference found.

%% Sanity checks to measure the correctness of the synchronization was
%% done through visual checks of the video frames to see if they were
%% indeed showing the purported frame.

\section{Results} \label{results}

% synchronization results
The results for the synchronization experiments show a total score of
0 for all trials, thus no frames were missed.  This result
demonstrates that our choices for the baseline client quality levels
and the throttled bandwidths do not push the clients beyond their
bandwidth resource capacities.

% don't know if this should stay in.
% i got these numbers by manually inspecting the files where the
% clients were playing at a higher level then their alloted
% bandwidth resources and dividing the missed frames by the number
% of frames that are supposed to be shown the 5 minutes (=14)
To show the effect of a negligent choice of playing levels, we also
ran the clients with the same bandwidth resources as above but at an
arbitrary quality level.  We chose the median quality level as the
video quality playing level and found that all clients with lower
bandwidth resources missed an average of 63\% of the needed frames.
Clients with resources equal to or above the required bandwidth for
the median quality level did not miss any frames.  In contrast, when
ran with the feedback controller, the same clients that missed the
large proportion of the frames only missed 35\% of the needed frames.

% qos results
The evaluation of the quality of service experiments show the baseline
clients scored a group score of 1 while the clients assisted by the
feedback controller scored a group score of 1.25.  The one-tailed
t-score of this difference is 3.01 which is significant for an
$\alpha$ value of .005 (N=17).  This result demonstrates that using
the feedback controller, we are able to achieve a significant positive
difference in the quality of services.  Note that these results do not
measure the degree of the positive difference achieved by the feedback
controller.  To demonstrate the degree of benefit of using the
feedback controller, we measure the proportion of additional frames
that each client maintained by the controller is able to enjoy.  We
found that those clients received 20.4\% ($\pm$ 9.7) more frames then
the clients operating at a baseline rate.

In the assessment of the risk of optimizing the frame rate we found
that there was only one instance in which the controller assisted
client missed two consecutive frames.  Upon closer inspection, the
time region during this event showed that the video demanded a higher
frame rate while the network bandwidth was relatively low.

% calculation used for the 20% number I got up there.
% baselineFrames = number of frames base client gets
% wfFrames = number of frames the wf client gets
% (wfFrames - baselineFrames) / baselineFrames = proportion of frames higher
%                                                then the baseline client

\section{Discussion} \label{discussion}
%% ??? don't know if we should bring this up.  need to discuss with
%% hellerstein about this.  
%%
%% Liu et al. also discusses convergence..., need to look up:
%% RLM and RLC (examples using this metric)

The main goals of ai2tv are distributed client video synchrony while
providing quality video.  We induced the latency delays by a per
client throttling of bandwidth at the server.  We show in our results
that, if the client chooses a quality level appropriate to its
bandwidth resources, the use of the publish subscribe communication
paradigm is sufficient in maintaining synchrony for multiple clients.

Though in some cases, using this system without the feedback
controller may be sufficient, but in most cases the network bandwidth
may vary and the variable frame rate of the video do not permit the
client to make an informed decision about the most appropriate quality
level.  This situation is modeled in the ``bad situation'' (need a
better phrase) experiment where the client chose a video quality
higher than the alloted bandwidth.  In addition, an application that
does not adjust its quality level to current bandwidth resources will
not be able to offer a level of quality appropriate to the client's
resources.  To address these issues, the feedback controller provides
an additional adaptive element to the clients.  We show that the
feedback controller makes a significant positive difference in aiding
the client in achieving a higher quality level.  

Liu et al. provide a comprehensive summary of the mechanisms used in
video multicast for quality and fairness adaptation as well as network
and coding requirements \cite{LIU}.  To frame our work that context
provided, our current design models a single-rate server adaptation
scheme to each of the clients because the video quality we provide is
tailored specifically to that client's network resources.  The focus
in our work was focused on the client side end user perceived quality
and synchrony, so we did not utilize the most efficient server model.
The authors believe that it would be trivial to substitute in a
simulcast server adaptation model.  Our design also fits into the
category of layered adaptation.  This adaptation model defines a base
quality level that users must achieve.  Once users have acquired that
level, the algorithm attempts to incrementally acquire more frames to
present a higher quality video.  In the work presented here, the
definition of quality translates to a higher frame rate.  In other
work (discussed in \ref{related}), the definition of quality may be
defined as the actual resolution of the received frames.  Liu's
discussion of bandwidth fairness, coding techniques and network
transport perspectives lie out of the scope of this paper.

Also in the realm of action control, Vogel et al. present a middleware
for consistency control in collaborating groups for distributed
interactive media.  \cite{VOGEL}.  He introduces the ideas of
optmistic and pessimistic mechanisms in insuring this consistency.
Optmistic mechanisms allow inconsistencies to happen and then deal
with the afterwards.  Pessimistic mechanisms prevent inconsistencies
by locking central state.  In terms of this framework, we provide
pessimistic mechanisms

\section{Related Work} \label{related}

A preliminary design and implementation of these project goals was
completed in VECTORS \cite{VECTORS}.  This system uses a different
synchronization scheme and is tightly coupled with a collaborative
virtual environment (CVE) called CHIME \cite{CHIME}.  Due to the
decentralized synchronization scheme employed in this design and
because of the heavy weight nature of the CVE, the system failed to
provide an adequate level of synchronziation.

% ----------------------------
% education related support:
% ----------------------------
Other recent support for online educational support are by Guerri et
al who present a real-time e-learning system based on global satellite
for data transfer from teacher to students, and JMF and Windows Media
\cite{GUERRI} to support the distribution and display of the video
content.  Their goal is simliar to ours because their work also
attempts to make education accessible to distanced communities.  Liu
presents the the WSML system \cite{LIU2}.  This system is a web-based
synchronization multimedia lecture system for the production of a
multimedia lecture.  The synchronization referred to in this context
is that of intermedia communication to provide a replicable playback
environment.

% -------------------------------
% multicast to several clients same video content
% -------------------------------
Similar to our ability to show the same content on multiple displays,
Choi et al. show a synchronization method for real time surround
display using clustered systems \cite{CHOI} but this system is
drastically different from ours because they assume a large
communication bandwidth and homogenous end systems.

% ----------------------------
% video quality of services
% ----------------------------
Another area of related work is the use of multi-level video summaries
and their traversal in the adaptation of the video stream to provide
video quality of services
\cite{CUI,KRASIC,LEI,NEUMANN,SHIPMAN,TAN,THAKUR}.  These systems don't
provide multiple client synchronized action control, but the
contributions made in these works could be used to optimize the
content distribution server.

% ---------------------
% shared action control
% ---------------------
Also related to our work is a collection of work related to the
concept of shared action control.  Song et al. \cite{SONG} provide a
type of remote video camera action control.  Liao et al. provide a
system for shared interactive video for teleconferencing \cite{LIAO}.
In this system, users are able to interact with the remote meeting
environment such as camera view or presentation direction.  These
systems support a notion of media interaction proposed by Stenzler et
al that purports that a video application is interactive if the user
can affect the flow of the video and that influence in turn, affects
the user's future choices \cite{STENZLER}.  We support this notion by
allowing all users to have the ability to control the flow of the
video.  Also in the realm of action control, Vogel et al. present a
middleware for consistency control in collaborating groups for
distributed interactive media.  \cite{VOGEL}.

% ----------------------------
%% do we need to have related work wrt to the semantic compression stuff, or
%% is that out of the scope of the paper?
%
%% other semantic compression algs: can we simply say that our choice is arbitrary?
%% - phung: High level segmentation of instructional videos based on content density 
%% - Wang: Learning-based linguistic indexing of pictures with 2--d MHMMs
%% - Tarel: On the choice of similarity measures for image retrieval by example
%% - Jing: An effective region-based image retrieval framework
%% - Goh: DynDex: a dynamic and non-metric space indexer
%% - slaney: Multimedia edges: finding hierarchy in all dimensions,
%% automating the table of contents.

\section{Future Work} \label{con}
Move to a multi-cast push model with the server.  Integrate streaming
audio.  Add smarter video content discovery mechanism.  Integrate into
CVE.  conduct user experiments....


\section{Conclusion}

In this paper we present a novel educational learning tool to allow
geographically dispersed participants to collaboratively view a video
in synchrony.  The system also employs a feedback controller
architecture that adapts the video quality according to client network
bandwidth resources.  The other novel concept that we put forth is the
use of semantic compression to facillitate the synchronization of
video content to clients with heterogenous resources.  We rely on the
semantic compression algorithm to guarantee the semantic composition
of the video frames is similar for all clients.  We then distribute
appropriate versions of the video to clients according to their
current bandwidth resources.  Through the use of these tools, we hope
to close the gap between students with varying network resources to
allow collaboration to proceed in a fruitful manner.

%ACKNOWLEDGMENTS are optional
% \section{Acknowledgments}
% Professor Kender and Tiecheng Liu at the High-Level Vision Lab.
% Matias Pelenur also helped a whole damn lot!!!

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ai2tv}
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

% ??? we'll need to do this right before submission
% \subsection{References}
% 
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.

% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.

\balancecolumns % GM July 2000
% That's all folks!
\end{document}
% ---------------------------------------------------------------
% / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
% / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
% / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
% ---------------------------------------------------------------
